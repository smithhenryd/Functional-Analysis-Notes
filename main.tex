\documentclass[11pt]{article}

% Mathematical typesetting
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{bbm}
\usepackage{mathrsfs}
\usepackage{centernot}

% Document formatting
\usepackage[utf8]{inputenc}
\usepackage{comment}
\usepackage[shortlabels]{enumitem}
\usepackage{hyperref}
\usepackage{csquotes}
\usepackage{abstract}
\usepackage{array}
\usepackage{float}
\usepackage{caption}

\usepackage{pgfplots}
\usepgfplotslibrary{fillbetween}


%% Remove abstract text
\renewcommand{\abstractname}{\vspace{-\baselineskip}} 

\usepackage[]{geometry}
\geometry{margin=1in, headsep=0.25in}
\newlength{\tabcont}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
\usepackage{framed}
\usepackage[most]{tcolorbox}
\usepackage{xcolor}

\newtheoremstyle{mystyle}{}{}{}{}{\bfseries}{:}{ }{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
  
\theoremstyle{mystyle}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lm}{Lemma}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{note}{Note}[section]

\newtheorem{protoexamp}{Example}[section]
\newenvironment{examp}
{\colorlet{shadecolor}{orange!15}\begin{shaded}\begin{protoexamp}}
{\end{protoexamp}\end{shaded}}

\newtheorem{protoexer}{Exercise}[section]
\newenvironment{exer}
{\colorlet{shadecolor}{blue!15}\begin{shaded}\begin{protoexer}}
{\end{protoexer}\end{shaded}}

\title{\textbf{Notes on Functional Analysis}}
\author{Henry Smith}
\date{\today}

\begin{document}

\maketitle
\begin{abstract}
    These notes are prepared from my reading of \textit{Introductory Functional Analysis with Applications} by Erwin Kreyszig. They are intended to serve as an aid for my understanding the book content. I do not guarantee their accuracy, and they do not reflect the accuracy of the book.
\end{abstract}

\newpage

\tableofcontents

\newpage

\section{Metric Spaces}

\subsection{Metric Spaces: Definition and Examples}

\begin{defn}[Metric Space]\label{metric}
A \textbf{metric space} is a pair $(X, d)$, where $X$ is a set and $d: X \times X \rightarrow \mathbb{R}_+$ is a \textbf{metric} on $X$. The metric $d$ must satisfy the following for all $x, y, z \in X$:
\begin{enumerate}
    \item $d(x, y) = 0 \iff x = y$
    \item $d(x, y) = d(y, x)$ \quad (symmetry)
    \item $d(x, z) \leq d(x, y) + d(y, z)$ \quad (triangle inequality)
\end{enumerate}
\end{defn}

A metric can be thought of as a measure of distance between any two elements $x, y \in X$. To illustrate this point, we provide some examples of metric spaces:

\begin{examp}[The real line $\mathbb{R}$] Let $X = \mathbb{R}$ and $d(x, y) := |x - y|$. Then $(X, d)$ is a metric space. This is our most classical notion of distance.
\end{examp}

\begin{examp}[The complex plane $\mathbb{C}$] Similarly, we let $X= \mathbb{C}$ and $d(x, y) := |x - y|$ for $x, y \in \mathbb{C}$. Recall that for $x = a + bi$, the modulus of $x$ is defined $|x| = \sqrt{a^2 + b^2}$. Then $(X, d)$ is a metric space.

\end{examp}

\begin{examp}[The Euclidean space $\mathbb{R}^n$, unitary space $\mathbb{C}^n$] For $X = \mathbb{R}^n$, we define $d(x, y) := \sqrt{(\xi_1 - \eta_1)^2 + \ldots + (\xi_n - \eta_n)^2 }$, where $x = (\xi_1, \ldots, \xi_n), y = (\eta_1, \ldots, \eta_n) \in \mathbb{R}^n$. $(X, d)$ defines the Euclidean metric space.\newline
Similarly, let $X = \mathbb{C}^n$ and define $d(x, y) := \sqrt{|\xi_1 - \eta_1|^2 + \ldots + |\xi_n - \eta_n|^2}$ for $x, y \in \mathbb{C}^n$. $(X, d)$ defines the unitary metric space.
\end{examp}

These are perhaps the most benign metric spaces that we can conjure from memory. Before presenting more exotic examples of metric spaces, we first discuss a few additional results:

\begin{lm}[Generalized Triangle Inequality]
Let $(X, d)$ be a metric space with $x_1, \ldots, x_n \in X$. Then it holds that
\begin{align*}
    d(x_1, x_n) \leq d(x_1, x_2) + \ldots + d(x_{n-1}, x_n).
\end{align*}
\end{lm}
This follows from an elementary induction argument.

\begin{defn}[Subspace]
Let $(X, d)$ be a metric space and $Y \subset X$ a subset of $X$. Then we can define a metric $\tilde{d}$ on $Y$ according to $\tilde{d} = d|_{Y \times Y}$. This is called the metric induced on $Y$ by $d$.
\end{defn}

The following are important examples of metric spaces that will be relevant throughout the remainder of our studies:
\begin{examp}[The sequence space $\ell^p$]\label{ellp}
For fixed $p \geq 1$, we define the space $\ell^p$ to consist of those complex sequences $x = (\xi_n)_{n=1}^{\infty} \subset \mathbb{C}$ such that
\begin{align*}
    \sum_{n=1}^{\infty}|\xi_n|^p < \infty.
\end{align*}
The metric defined on $\ell^p$ is 
\begin{align*}
    d(x, y) = \left( \sum_{n=1}^{\infty} |\xi_n - \eta_n|^p \right)^{1/p}, \qquad x = (\xi_n), \ y = (\eta_n) \in \ell^p.
\end{align*}
One can verify that indeed $d(x, y) < \infty$ using the \textit{Minkowski inequality}:
\begin{align*}
    \left( \sum_{n=1}^{\infty} | \xi_n + \eta_n|^p  \right)^{1/p} \leq \left( \sum_{n=1}^{\infty} | \xi_n|^p \right)^{1/p} + \left( \sum_{n=1}^{\infty} |\eta_n|^p \right)^{1/p}.
\end{align*}
Then $(\ell^p, d)$ defines a metric space. In the special case of $p = 2$, this is a \textbf{Hilbert space} (see Section \ref{}).
\end{examp}

\begin{examp}[The sequence space $\ell^{\infty}$]
Similar to the space considered in Example \ref{ellp}, we let $\ell^{\infty}$ be the set of all bounded sequences in $\mathbb{C}$. That is,
\begin{align*}
    \ell^{\infty} = \{ x = (\xi_n) \subset \mathbb{C} \ | \ | \xi_n | \ \leq c_x \ \text{for some $c_x \in \mathbb{R}_+$} \}.
\end{align*}
The metric we define on $\ell^{\infty}$ is $d(x, y) = \sup_{n \in \mathbb{N}} |\xi_n - \eta_n |$. $(\ell^{\infty}, d)$ defines a metric space.
\end{examp}

\begin{examp}[The function space $C{[a,b]}$]\label{contfun}
Let $X$ be the set of all real-valued continuous functions defined on the interval $[a, b]$. Then for each $x(t), y(t) \in X$, we define the metric $d(x, y) := \max_{t \in [a, b]} | x(t) - y(t) |$.\newline 
One should notice that we use \enquote{max} rather than \enquote{sup} in our definition of the metric $d$. This is because $f(t) = |x(t) - y(t)|$ is continuous on the closed interval $[a, b]$ and thus attains its maximum on this interval (the Weierstrass Extreme Value Theorem).\newline
This defines the metric space $C[a, b]$.
\end{examp}

\begin{examp}[The discrete metric space]\label{discretemetric}
For any set $X$, define the metric $d$ such that
\begin{align*}
    d(x, y) = 
    \begin{cases}
    1 & \text{if $x=y$} \\
    0 & \text{otherwise}
    \end{cases}.
\end{align*}
Then $(X, d)$ is called the \textbf{discrete metric space}.
\end{examp}

\begin{exer}
If $A$ is the subspace of $\ell^{\infty}$ consisting of all zeros and ones, what is the induced metric on $X$?\newline
The induced metric $\tilde{d}$ on $A$ is the discrete metric (see Example \ref{discretemetric}).
\begin{proof}
Let $x = (\xi_n), y = (\eta_n) \in A$. Then if $x \neq y$, it must be the case that $|\xi_j - \eta_j| = 1$ for some $j \in \mathbb{N}$. By the definition of $(\ell^{\infty}, d)$, this implies $\tilde{d}(x, y) = 1$. Also, by the properties of a metric space, we know $\tilde{d}(x, x) = 0$. We deduce that $\tilde{d}$ is the discrete metric.
\end{proof}
\end{exer}

\begin{exer}
Show that a metric on the space $X$ from Example \ref{contfun} is 
\begin{align*}
    \tilde{d}(x, y) = \int_a^b |x(t) - y(t)| dt.
\end{align*}

\begin{proof}
First, notice that since $x, y \in X$, then $|x - y| \in X$. That is, $|x(t) - y(t)|$ is continuous over $[a, b]$, and is thus Riemann integrable. This means that $\tilde{d}$ is well-defined. Similarly, we have
\begin{enumerate}
    \item $|x(t) - y(t)| \geq 0, \ t \in [a, b] \implies \tilde{d}(x, y) = \int_a^b |x(t) - y(t)| dt \geq 0$
    \item $|x(t) - y(t)| \leq c, \ \forall t \in [a, b]$ for some $c \in \mathbb{R}_+$ $\implies \tilde{d}(x, y) \leq c \cdot (b -a) \in \mathbb{R}_+$
\end{enumerate}
Therefore, it is indeed the case that $\tilde{d}: X \times X \rightarrow \mathbb{R}_+$.\newline
To verify axiom (1), we clearly see that $x = y$ implies $\tilde{d}(x, y) = 0$. For the opposite implication, suppose $x(t_0) \neq y(t_0)$ for some $t_0 \in [a, b]$. Without loss of generality, suppose $t_0 \notin \{a, b\}$. Then by the continuity of $x$ and $y$, this implies that $|x(t) - y(t)| > 0$ for all $t \in [t_0 - \varepsilon, t_0 + \varepsilon] \subseteq [a, b]$, where $\varepsilon > 0$ is chosen to be sufficiently small. Accordingly, we have
\begin{align*}
    &\tilde{d}(x, y) = \underbrace{\int_a^{t_0 - \varepsilon} |x(t) - y(t)| dt}_{\geq 0} + \underbrace{\int_{t_0 - \varepsilon}^{t_0 + \varepsilon} |x(t) - y(t)| dt}_{> 0} + \underbrace{\int_{t_0 - \varepsilon}^{b} |x(t) - y(t)| dt}_{\geq 0}\\
    \implies&\tilde{d}(x, y) > 0.
\end{align*}
Symmetry (2) of the metric $\tilde{d}$ follows from the properties of the absolute value.\newline
Finally, to prove the triangle inequality (3), we have that for $x, y, z$ continuous functions on $[a, b]$
\begin{align*}
    \tilde{d}(x, z) &= \int_a^b | x(t) - z(t) | dt \leq \int_a^b \bigg( | x(t) - y(t) | + | y(t) - z(t) |\bigg) dt\\
    &= \int_a^b | x(t) - y(t) | dt + \int_a^b | y(t) - z(t) | dt\\
    &= \tilde{d}(x, y) + \tilde{d}(y, z).
\end{align*}
\end{proof}
\end{exer}

\begin{exer}
Show that nonnegativity of a metric follows from axioms (1) - (3).

\begin{proof}
Let $(X, d)$ be a metric space and $x, y \in X$ arbitrary points. Then by (3) we have
\begin{align*}
    d(x, x) \leq d(x, y) + d(y, x).
\end{align*}
Property (1) implies
\begin{align*}
    0 \leq d(x, y) + d(y, x).
\end{align*}
And property (2) implies
\begin{align*}
    0 \leq 2d(x, y) \iff 0 \leq d(x, y).
\end{align*}
\end{proof}
\end{exer}

\begin{exer}\label{sequencespaces}
Define $s$ to be the space of all (bounded or unbounded) sequences of complex numbers and the metric $d$ defined by
\begin{align*}
    d(x, y) = \sum_{n=1}^{\infty} \frac{1}{2^n} \frac{|\xi_n - \eta_n|}{1 + |\xi_n - \eta_n|}, \qquad x = (\xi_n), \ y= (\eta_n) \in s.
\end{align*}
Show that we can obtain another metric by replacing $(1/2^n)$ with $(\mu_n), \ \mu_n > 0$ such that $\sum \mu_n$ converges.
\begin{proof}
First, we point out that $\mu_n \cdot \frac{|\xi_n - \eta_n|}{1 + |\xi_n - \eta_n|} \geq 0$ for each $n \in \mathbb{N}$, and so $\tilde{d}(x, y) = \sum_{n=1}^{\infty} \mu_n \cdot \frac{|\xi_n - \eta_n|}{1 + |\xi_n - \eta_n|} \geq 0$. Also we notice that $\mu_n \cdot \underbrace{\frac{|\xi_n - \eta_n|}{1 + |\xi_n - \eta_n|}}_{ < 1} < \mu_n$ for each $n \in \mathbb{N}$, and so $\tilde{d}(x, y) = \sum_{n=1}^{\infty} \mu_n \cdot \frac{|\xi_n - \eta_n|}{1 + |\xi_n - \eta_n|} < \sum_{n=1}^{\infty} \mu_n < \infty$.\newline
Now, for (1) we have 
\begin{align*}
    &\tilde{d}(x, y) = 0 \iff \sum_{n=1}^{\infty} \underbrace{\mu_n \cdot \frac{|\xi_n - \eta_n|}{1 + |\xi_n - \eta_n|}}_{\geq 0} = 0 \iff \mu_n \cdot \frac{|\xi_n - \eta_n|}{1 + |\xi_n - \eta_n|} = 0, \ n \in \mathbb{N}\\
    \iff& \frac{|\xi_n - \eta_n|}{1 + |\xi_n - \eta_n|} = 0, \ n \in \mathbb{N} \iff x = y.
\end{align*}
(2) follows simply from the properties of the modulus function.\newline
Finally, for the triangle inequality (3), we let $z = (\gamma_n) \in s$ and use the result from pp. 10 - 11:
\begin{align*}
    &\frac{|\xi_n - \gamma_n|}{1 + |\xi_n - \gamma_n|} \leq \frac{|\xi_n - \eta_n|}{1 + |\xi_n - \eta_n|} + \frac{|\eta_n - \gamma_n|}{1 + |\eta_n - \gamma_n|}, \quad n \in \mathbb{N}\\
    \iff&\mu_n \cdot \frac{|\xi_n - \gamma_n|}{1 + |\xi_n - \gamma_n|} \leq \mu_n \cdot \frac{|\xi_n - \eta_n|}{1 + |\xi_n - \eta_n|} + \mu_n \cdot \frac{|\eta_n - \gamma_n|}{1 + |\eta_n - \gamma_n|}, \quad n \in \mathbb{N}\\
    \implies& \sum_{n=1}^{\infty} \mu_n \cdot \frac{|\xi_n - \gamma_n|}{1 + |\xi_n - \gamma_n|} \leq \sum_{n=1}^{\infty} \mu_n \cdot \frac{|\xi_n - \eta_n|}{1 + |\xi_n - \eta_n|} + \sum_{n=1}^{\infty} \mu_n \cdot \frac{|\eta_n - \gamma_n|}{1 + |\eta_n - \gamma_n|}\\
    \iff& \tilde{d}(x, z) \leq \tilde{d}(x, y) + \tilde{d}(y, z).
\end{align*}
We deduce $(s, \tilde{d})$ is a metric space.
\end{proof}
\end{exer}

\begin{exer}
The \textbf{diameter} $\delta(A)$ of a nonempty set $A$ in a metric space $(X, d)$ is defined to be 
\begin{align*}
    \delta(A) = \sup_{x, y\in A} d(x, y).
\end{align*}
$A$ is said to be \textbf{bounded} if $\delta(A) < \infty$. Prove each of the following:
\begin{enumerate}
    \item $A \subset B$ implies $\delta(A) \leq \delta(B)$.
    \item $\delta(A) = 0 \iff$ $A$ consists of a single point.
\end{enumerate}

\begin{proof}
(1) This follows from the properties of the the supremum. Namely, 
\begin{align*}
    &C_1=\{d(x, y) \ | \ x, y \in A \} \subset C_2=\{d(x, y) \ | \ x, y \in B \}\\
    \implies& \sup C_1 \leq \sup C_2\\
    \iff& \delta(A) \leq \delta(B).
\end{align*}
(2) 
\begin{align*}
    &\delta(A) = 0 \iff \sup_{x, y\in A} d(x, y) = 0 \iff d(x, y) = 0, \ \forall x, y \in A\\
    &\iff\text{A consists of a single point}
\end{align*}
\end{proof}
\end{exer}

\begin{exer}
The \textbf{distance} $D(A, B)$ between two nonempty subsets $A$ and $B$ of a metric space $(X, d)$ is defined to be 
\begin{align*}
   D(A, B) = \inf_{\substack{a \in A\\b \in B}} d(a, b). 
\end{align*}
Show each of the following:
\begin{enumerate}
    \item $D$ does \textit{not} define a metric on the power set of $X$.
    \item If $A \cap B \neq \varnothing$, then $D(A, B) = 0$. What about the converse?
\end{enumerate}

\begin{proof}
(1) Let $P, Q \in 2^X$ such that $P \neq Q$ and $P \cap Q \neq 
\varnothing$. Notice that $d$ is a metric, and so $d(p, q) \geq 0, \ \forall p \in P, \ \forall q \in Q$. This implies $D(P, Q) \geq 0$. Moreover, $P \cap Q \neq \varnothing$ implies $D(P, Q) \leq 0$. Altogether, we have shown $D(P, Q) = 0$ but $P \neq Q$. Therefore, we conclude $D$ does not define a metric.\newline
(2) Since $A \cap B \neq \varnothing$, then $0 \in \{ d(a, b) \ | \ a \in A, \ b \in B\}$. This tells us that $D(A, B) \geq 0$. And by our argument from (1), $d(a, b) \geq 0, \ \forall a \in A, \ \forall b \in B$, which implies $D(A, B) = \inf_{\substack{a \in A\\b \in B}} d(a, b) \geq 0$. We conclude $D(A, B) = 0$.\newline
The converse is not true. Consider the Euclidean metric space $(\mathbb{R}^2, d)$. Let $A = \{(x, 0) \ | \ x > 0 \}$ be the positive $x$-axis and $B = \{(x, y) \ | \ x >0, \ y \geq x^{-1} \}$ the region bounded by the curve $y = x^{-1}$. Since $x^{-1} > 0$ for every $x > 0$, then $A \cap B = \varnothing$. Consider the sequence $(a_n) \subset A$ defined $a_n = (n, 0)$ as well as the sequence $(b_n) \subset B$ defined $b_n = (n, n^{-1})$. Then $d(a_n, b_n) = n^{-1}$ for all $n \in \mathbb{N}$, which proves that $D(A, B) = \inf_{\substack{a \in A\\b \in B}} d(a, b) \leq 0$. By nonnegativity of the metric $d$, we conclude $D(A, B) = 0$ but $A \cap B = \varnothing$.
\end{proof}

% Add plot
\begin{center}
\begin{tikzpicture}
\begin{axis}[samples=200, xlabel={$x$}, yticklabels={}]
\addplot[<->, semithick][name path=f, domain=.1:5] {1/x};
\node[label={$B$}] at (axis cs:2,5) {};
\path[name path=axis] (axis cs:0.1,10) -- (axis cs:5,10);
\addplot[gray!50] fill between[of=f and axis];
\node[circle, label={45:$A$}] at (axis cs:0,0) {};
\draw[->, semithick] (axis cs:0,0) -- (axis cs:5,0);
\end{axis}
\end{tikzpicture}
\end{center}
\end{exer}

\newpage
\begin{exer}\label{pttoset}
The \textbf{distance} $D(x, B)$ from a point $x$ to a nonempty subset $B$ of a metric space $(X, d)$ is defined to be
\begin{align*}
    D(x, B) = \inf_{b \in B} d(x, b).
\end{align*}
Show that for any $x, y \in X$, $|D(x, B) - D(y, B)| \leq d(x, y)$.
\begin{proof}
For each element $b \in B$, it holds that $d(x,b) \leq d(x, y) + d(y, b)$ by the triangle inequality. Consequently, we get the inequality 
\begin{align*}
    \inf_{b \in B} d(x, b) \leq d(x, y) + \inf_{b \in B} d(y, b) \iff D(x, B) - D(y, B) \leq d(x, y).
\end{align*}
By starting with $d(y, b) \leq d(y, x) + d(x, b)$, we similarly have $D(y, b) - D(x, b) \leq d(x, y)$. And so we conclude $|D(x, B) - D(y, B)| \leq d(x, y)$, as desired.
\end{proof}
\end{exer}

\subsection{Set Theory, Continuity, and Separability}

\subsubsection{Set Theory}\label{settheory}

\begin{defn}[Ball and sphere]
Let $(X, d)$ be a metric space. Given a point $x_0 \in X$ and a real number $r > 0$, we define three sets:
\begin{enumerate}
    \item \textbf{Open ball}: $B_r(x_0) = \{x \in X | d(x, x_0) < r \}$
    \item \textbf{Closed ball}: $\tilde{B}_r(x_0) = \{x \in X | d(x, x_0) \leq r \}$
    \item \textbf{Sphere}: $S_r(x_0) = \{x \in X | d(x, x_0) = r \}$
\end{enumerate}
\end{defn}

% Add plot
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\begin{axis}[samples=200, xlabel={$t$}, yticklabels={}, yticklabels={}]
\addplot[semithick][name path=f, domain=-2:2] {x^2};
\addplot[semithick, dashed][name path=g, domain=-2:2] {x^2 -1};
\addplot[semithick, dashed][name path=h, domain=-2:2] {x^2 +1};
\addplot[gray!50] fill between[of=g and h];
\end{axis}
\end{tikzpicture}
\end{center}
\caption*{The open unit ball $B_{r=1}(x)$ for $x(t) = t^2$ in the function space $C[-2, 2]$}
\end{figure}


\begin{defn}[Open set, closed set]
A subset $M$ of a metric space $X$ is said to be \textbf{open} if it contains a (open) ball about each of its points. A subset $K$ is said to be \textbf{closed} if its complement $X \setminus K$ is open.
\end{defn}

\begin{examp}\label{emptyopen}
In an arbitrary metric space $X$, the subsets $\varnothing$ and $X$ are both open and closed.
\end{examp}

\begin{examp}
In a discrete metric space $X$, every subset is both open and closed.
\begin{proof}
It suffices to show that for each $M \in 2^X$, then $M$ is open. One will recall that $\varnothing$ is open from Exercise \ref{emptyopen}, so we suppose $|M| > 0$. Let $m \in M$ be an arbitrary point in $M$. Then for each $r \in (0, 1]$, the open ball $B_r(x_0) = \{m \}$ is contained in the set $M$. This proves that $M$ is open, and we conclude that every subset of a discrete metric space is both open and closed.
\end{proof}
\end{examp}

\begin{defn}
Let $X$ be a metric space and $M$ a (nonempty) subset of $X$. Then $x_0 \in M$ is an \textbf{interior point} of the set $M$ if there exists some $r > 0$ such that $B_r(x_0) \subset M$. The \textbf{interior} of $M$, denoted $\text{Int}(M)$ is the set of all interior points of $M$.
\end{defn}

Immediately, we see that every point of an open set $M$ is an interior point, per our definition of an open set, and so $\text{Int}(M) = M$. Moreover, we have the following result which explains why we care about set interiors:

\begin{thm}\label{interior}
Let $X$ be a metric space and $M$ a nonempty subset of $X$. Then $\text{Int}(M)$ is the largest open set contained in $M$.
\end{thm}
\begin{proof}
First we prove that $\text{Int}(M)$ is a subset of $M$ and is open. By our definition, it is clear that $\text{Int}(M) \subseteq M$. Toward contradiction, suppose that $\text{Int}(M)$ is not open, meaning there exists some $x_0 \in \text{Int}(M)$ such that $B_r(x_0) \not\subset \text{Int}(M)$ for each $r > 0$. But this mean that $B_r(x_0) \not\subset M$ for each $r > 0$, which contradicts our assumption that $x_0 \in \text{Int}(M)$.\newline
We proceed to show that $\text{Int}(M)$ is the largest open set contained in $M$. Let $U$ be an arbitrary open set contained in $M$. Then for each $u \in U$ we have $u \in B_r(u) \subset U \subseteq M$, which implies $u \in \text{Int}(M)$. And so we conclude $U \subseteq \text{Int}(M)$.
\end{proof}

Having formally defined an open set for a metric space $(X, d)$, we give a more general notion of a space that satisfies the properties of open sets:
\begin{defn}
A \textbf{topological space} $(X, \mathscr{F})$ is a set $X$ and a collection $\mathscr{F}$ of subsets of $X$ such that $\mathscr{F}$ satifies the following axioms:
\begin{enumerate}
    \item $\varnothing, X \in \mathscr{F}$ 
    \item $\{F_n\}_{n \in \mathcal{I}} \subseteq \mathscr{F} \implies \bigcup_{n \in \mathcal{I}} F_n \in \mathscr{F}$ \quad ($\mathscr{F}$ is closed under arbitrary unions)
    \item $\{F_n\}_{n=1}^N \subseteq \mathscr{F} \implies \bigcap_{n=1}^N F_n \in \mathscr{F}$ \quad ($\mathscr{F}$ is closed under finite intersections)
\end{enumerate}
\end{defn}

It is evident that each metric space $X$ is a topological space with $\mathscr{F}$ the collection of all open subsets of $X$.

\begin{note}
One should be careful in recognizing that open sets, and thus topological spaces, \textit{are not closed under countable intersections}. For a straightforward counterexample, let $(\mathbb{R}, d)$ be the Euclidean metric space and take $\{F_n \}_{n \in \mathbb{N}}$ where each $F_n := (-n^{-1}, n^{-1})$ is an open set. Then $\cap_{n \in \mathbb{N}} F_n = \{ 0 \}$ is not open.
\end{note}

\begin{defn}
Let $X$ be a metric space and $M$ a (nonempty) subset of $X$. Then a point $x$ of $X$, which may or may not be a point of $M$, is called an \textbf{accumulation point} or \textbf{limit point} of $M$ if for every $\varepsilon > 0$, $B_{\varepsilon}(x)$ contains a point $m_{\varepsilon}\in M$ distinct from $x$. The set consisting of the points of $M$ and the limit points of $M$ is called the \textbf{closure} of $M$ and is denoted by $\bar{M}$.
\end{defn}

\begin{thm}
Let $X$ be a metric space and $M$ a nonempty subset of $X$. Then $\bar{M}$ is the smallest closed set containing $M$.
\end{thm}
\begin{proof}
Similar to the structure of our proof of Theorem \ref{interior}, we first show that $\bar{M}$ contains $M$ and is closed. The first statement is a consequence of the definition of the closure. As for the latter, let $x_0 \in X \setminus \bar{M}$ be an arbitrary point. Suppose towards contradiction that there exists no $\varepsilon > 0$ such that $B_{\varepsilon}(x_0) \subset X \setminus \bar{M}$. Then for every $\varepsilon > 0$, there exists some $m_{\varepsilon} \in M$ such that $m_{\varepsilon} \in B_{\varepsilon}(x_0)$. However, this means that $x_0$ is a limit point of the set $M$, and so $x_0 \in \bar{M}$, a contradiction.\newline
Now we show that $\bar{M}$ is the smallest closed set containing $M$. In particular, let us consider an arbitrary closed set $U$ containing $M$. And choose an arbitrary point $m_0 \in \bar{M}$. Suppose towards contradiction that $m_0 \notin U \implies m_0 \in X \setminus U$. Also notice that since $M \subseteq U$, then the point $m_0$ cannot be contained in $M$. Thus, $m_0$ must be a limit point of $M$. This, however, is a contradiction, since it implies that $B_{\varepsilon}(m_0)$ contains a point $m_{\varepsilon} \in M \subseteq U$ for each $\varepsilon > 0$, and so $X \setminus U$ cannot be open. Thus, it must be the case that $\bar{M} \subseteq U$ for each closed set $U$ containing $M$.
\end{proof}

\subsubsection{Continuity}

\begin{defn}[Continuous mapping]
Let $(X, d)$ and $(Y, \tilde{d})$ be metric spaces. A mapping $T: X \longrightarrow Y$ is said to be \textbf{continuous at a point} $x_0 \in X$ if for every $\varepsilon > 0$, there exists some $\delta > 0$ such that
\begin{align*}
    d(x, x_0) < \delta \quad \implies \quad \tilde{d}(Tx, Tx_0) < \varepsilon.
\end{align*}
$T$ is said to be \textbf{continuous} if it is continuous at all points $x \in X$.
\end{defn}

Continuous mappings have an important connection with open sets as summarized by the following theorem:
\begin{thm}[Continuous Mapping]
A mapping $T$ of a metric space $X$ into a metric space $Y$ is continuous if and only if the inverse image of any open subset of $Y$ is an open subset of $X$.
\end{thm}
\begin{proof}
$\implies$ Suppose $T$ is continuous, and let $A$ be any open subset of $Y$.
Further, let $B = T^{-1}A$ be the inverse image of $A$. Consider an arbitrary $b_0 \in B$. Then $Tb_0 \in A$. By assumption that $A$ is open, there exists some $\varepsilon > 0$ such that $B_{\varepsilon}(Tb_0) \subset A$. And since $T$ is continuous, there exists some $\delta > 0$ such that $d(b, b_0) < \delta \implies \tilde{d}(Tb, Tb_0) < \varepsilon$. But because $B_{\varepsilon}(Tb_0) \subset A$, this implies that the ball $B_{\delta}(b_0)$ is contained in $B$.\newline
$\impliedby$ Conversely, suppose the inverse image of any open subset of $Y$ is an open subset of $X$. Let us take $x_0 \in X$ to be arbitrary. Then for each $\varepsilon > 0$, $B_{\varepsilon}(Tx_0)$ is an open set. By our assumption, this means that the inverse image of  $B_{\varepsilon}(Tx_0)$, denoted $A$, is open. Consequently, by the definition of $A$ an open set, there exists some $\delta > 0$ such that $B_{\delta}(x_0) \subset A$. Thus we conclude $d(x, x_0) < \delta \implies \tilde{d}(Tx, Tx_0) < \varepsilon$.
\end{proof}

\subsubsection{Separability}
The set theory we have built up in Section \ref{settheory} leads us to the important concept of dense subsets and separable spaces which will be important throughout the remainder of our studies:
\begin{defn}[Dense set, separable space]
A subset $M$ of a metric space $X$ is said to be \textbf{dense} in $X$ if $\bar{M} = X$. $X$ is said to be \textbf{separable} if it has a \textit{countable} subset which is dense in $X$.
\end{defn}

Consequently, if $M$ is dense in $X$, then every ball in $X$ (with nonzero radius), no matter how small, will contain a point of $M$.

\begin{examp}[Weierstrass Approximation Theorem]\label{weierstrass}
The set of polynomial functions defined on the interval $[a, b] \subset \mathbb{R}$ is dense in the metric space $C[a, b]$.\footnotemark
\end{examp}\footnotetext{My favorite proof of the Weierstrass Approximation Theorem is that which relies upon \href{https://en.wikipedia.org/wiki/Fej\%C3\%A9r\%27s_theorem}{Fejer's Theorem}.}

\begin{examp}
$\mathbb{R}$ is separable.
\begin{proof}
$\mathbb{Q}$ is a countable, dense subset of $\mathbb{R}$.
\end{proof}
\end{examp}

\begin{examp}
$\mathbb{C}$ is separable.
\end{examp}

\begin{examp}
$\ell^{\infty}$ is \textbf{not} separable. $\ell^P$ is separable for $1 \leq p < \infty$.
\begin{proof}
We prove that $\ell^p$ is separable.\newline
Let us define the set 
\begin{align*}
    M_n = \{ (\eta_1, \ldots, \eta_n, 0, 0,\ldots ) \ | \  \eta_i \in \mathbb{Q} \ \text{for} \ 1 \leq i \leq n\}
\end{align*}
which consists of those sequences whose first $n$ terms are rational and remaining terms are zero. Each of these sequences is in $\ell^p$ since it only has finitely many nonzero terms. Note that $M_n$ is countable since it is equal to the Cartesian product of countable sets.\newline
From here the set
\begin{align*}
    M = \bigcup_{n \in \mathbb{N}} M_n
\end{align*}
is countable because it is a countable union of countable sets.\newline
We claim that $M$ is dense in $\ell^{p}$. To prove this, take an arbitrary element $x = (\xi_1, \xi_2, \ldots) \in \ell^p$. Then for each $\varepsilon > 0$, there must exist some $N_{\varepsilon}$ such that $\sum_{n > N_{\varepsilon}} |\xi_i|^p < \frac{\varepsilon^p}{2}$ (by definition of the space $\ell^p$). And since $\mathbb{Q}$ is dense in the Euclidean metric space $\mathbb{R}$, for each $\xi_n$, $1 \leq n \leq N_{\varepsilon}$ there exists an $\eta_n$ such that $|\xi_n - \eta_n| < \frac{\varepsilon^p}{2^{n+1}}$.\newline
Let us define $w = (\eta_1, \ldots, \eta_n, 0, 0 \ldots,) \in M$. Then we have that
\begin{align*}
    d(x, w) &= \left( \sum_{n=1}^{\infty} |\xi_n - \eta_n|^p \right)^{1/p}
    = \left( \sum_{n=1}^{N_{\varepsilon}} |\xi_n - \eta_n|^p + \sum_{n > N_{\varepsilon}} |\xi_n|^p \right)^{1/p}\\
    &\leq \left( \frac{\varepsilon^p}{2} + \frac{\varepsilon^p}{2} \right)^{1/p} = \varepsilon.
\end{align*}
Because $\varepsilon > 0$ was chosen arbitrarily, we deduce that $M$ is indeed dense in $\ell^p$ for each $1 \leq p < \infty$, and so $\ell^p$ is separable.
\end{proof}
\end{examp}

\begin{exer}
Consider $C[0, 2\pi]$ and determine the smallest $r$ such that $y \in \tilde{B}_r(x)$, where $x(t) = \sin(t)$ and $y(t) = \cos(t)$.\newline
First, we comment that
\begin{align*}
    y \in \tilde{B}_r(x) \iff \max_{t \in [0, 2\pi]} |\sin(t) - \cos(t)| \leq r.
\end{align*}
And so we must maximize the function $f(x) = |\sin(t) - \cos(t)|$ over the interval $[0, 2\pi]$. To do so, we find the extreme points of the function $\tilde{f}(t) = \sin(t) - \cos(t)$:
\begin{align*}
    &\tilde{f}'(t) = 0 \iff \frac{d}{dt}(\sin(t) - \cos(t)) = 0\\
    &\implies \cos(t) + \sin(t) = 0 \implies t \in \bigg\{ \frac{3\pi}{4}, \frac{7 \pi}{4} \bigg\}.
\end{align*}
$t = \frac{3\pi}{4}$ is a maximizer of $\tilde{f}$, whereas $t = \frac{7\pi}{4}$ is a minimizer of $\tilde{f}$. Plugging each of these values for $t$ into the original objective $f$, we get that $\max_{t \in [0, 2\pi]} |\sin(t) - \cos(t)| = \sin\left( \frac{3\pi}{4} \right) - \cos \left(\frac{3\pi}{4} \right) = \sqrt{2}$. We conclude that the smallest value of $r$ such that  $y \in \tilde{B}_r(x)$ is $r = \sqrt{2}$.
\end{exer}

\begin{exer}
If $x_0$ is a limit point of a set $A \subset (X, d)$, show that any neighborhood of $x_0$ contains infinitely many points of $A$. A \textbf{neighborhood} is defined to be any set which contains $B_{r}(x_0)$ for some $r > 0$.
\begin{proof}
Let $M$ be an arbitrary neighborhood of $x_0$. Suppose towards contradiction that $M$ contains only finitely many points of $A$. Let $a_1, \ldots, a_N$ be the points of $A$ contained in $M$ which are distinct from $x_0$. Since $M$ is a neighborhood of $x_0$, then $B_r(x_0) \subset M$ for some $r > 0$. Let $r^* = \min\{d(x_0, a_1), \ldots, d(x_0, a_N), r\}$. Then $B_{r^*}(x_0) \subset M$ and $B_{r^*}(x_0)$ does not contain any points of $A$ distinct from $x_0$. This, though, contradicts our assumption that $x_0$ is a limit point of the set $A$. Note that we needed $M$ to be a neighborhood of $x_0$ since this implies $r > 0$.
\end{proof}
\end{exer}

\begin{exer}
A point $x$ not belonging to a closed set $M \subset (X, d)$ always has a nonzero distance from M (as defined in Exercise \ref{pttoset}). To prove this, show that $x \in \bar{A}$ if and only if $D(x, A) = 0$; here, $A$ is any nonempty subset of $X$.
\begin{proof}
$\implies$ First let us suppose that $x \in \bar{A}$. If $x \in A$, then $D(x, A) = 0$. Otherwise, $x$ must be a limit point of the set $A$. Accordingly, there exists a sequence of points $\{a_n\}_{n\in \mathbb{N}}$, $a_n \in A$ such that $a_n \in B_{n^{-1}}(x) \iff d(x, a_n) < n^{-1}$. Therefore, we have the sequence $\{d(x, a_n) \}_{n \in \mathbb{N}}$ with $\lim_{n \to \infty} d(x, a_n) = 0$. This implies $D(x_0, A) = 0$.\newline
$\impliedby$ Otherwise, suppose $D(x, A) = 0$. By definition of the infimum, for every $\varepsilon > 0$, there exists an $a_{\varepsilon} \in A$ such that $d(x, a_{\varepsilon}) < D(x, A) + \varepsilon = \varepsilon$. However, this then implies $a_{\varepsilon} \in B_{\varepsilon}(x)$ for every $\varepsilon > 0$. We conclude that $x$ must be a limit point of $A$, and so $x \in \bar{A}$. 
\end{proof}
\end{exer}

\begin{exer}[The function space $B(A)$]
Let $X$ consist of all functions that are bounded on a set $A$. And define the metric function
\begin{align*}
    d(x, y) = \sup_{t \in A}|x(t) - y(t)| \quad x, y \in B(A).
\end{align*}
$(X, d)$ defines the metric space $B(A)$.\newline
Show that $B[a, b]$, $a < b$, is not separable.

\begin{proof}
Let us consider the set of functions
\begin{align*}
    F = \bigcup_{x \in [a, b]\cap(\mathbb{R} \setminus \mathbb{Q})} \mathbbm{1}_x,
\end{align*}
where
\begin{align*}
    \mathbbm{1}_x(t) = \begin{cases}
    1 & \text{if $t = x$}\\
    0 & \text{otherwise}
    \end{cases}.
\end{align*}
We make two comments regarding this set $F$. First, $F \subset B[a, b]$ since for each $f \in B[a, b]$, $\sup_{t \in [a, b]}|f(t)| = 1$. Also, our set $F$ contains uncountably many functions since the set $[a, b]\cap(\mathbb{R} \setminus \mathbb{Q})$ is uncountable.\newline
Now that we have defined this set $F$, let us consider $\mathbbm{1}_x, \mathbbm{1}_y \in F$ for $x \neq y$. In particular, we have $d(\mathbbm{1}_x,\mathbbm{1}_y) = |\mathbbm{1}_x(x) - \mathbbm{1}_y(x)| = 1$. And so if we let each function $\mathbbm{1}_x \in F$ be the center of an open ball $B_r(\mathbbm{1}_x)$ with radius $r \leq 1$, we have uncountably many balls that are pairwise disjoint. That is, we can cover the set $F$ by an uncountable collection of disjoint, open balls. Accordingly, if $M$ is any dense set in $B[a,b]$, each of the uncountably many balls must contain an element of $M$. Therefore, there can exist no countable, dense subset of $B[a, b]$.
\end{proof}
\end{exer}

\begin{exer}
Show that the image of an open set under continuous mapping need not be open.\newline
Let $X = Y = (\mathbb{R}, d)$ be the Euclidean metric space and define the map $T: X \longrightarrow Y$ according to $Tx := 0$ for every $x \in X$. Clearly $T$ is continuous because for every $x_0 \in X$ and for every $\varepsilon > 0$, $d(x, x_0) < 1 \implies d(Tx, Tx_0) = d(0, 0) = 0 < \varepsilon$. However, the image of the open set $X = \mathbb{R}$ under $T$ is $\{0\}$, a closed set in $(\mathbb{R}, d)$.
\end{exer}

\subsection{Convergence, Cauchy Sequences, and Completeness}

\begin{defn}
A sequence $(x_n)$ in a metric space $X=(X,d)$ is said to \textbf{converge} or to \textbf{be convergent} if there exists an $x \in X$ such that
\begin{align*}
    \lim_{n \to \infty} d(x_n, x) = 0.
\end{align*}
If this is the case, $x$ is called the \textbf{limit} of $(x_n)$ and we write \begin{align*}
    \lim_{n \to \infty} x_n = x \qquad \text{or} \qquad x_n \longrightarrow x.
\end{align*} 
We say the sequence $(x_n)$ \textbf{converges to $x$} or \textbf{has the limit} $x$. If $(x_n)$ is not convergent, it is said to be \textbf{divergent}.
\end{defn}

\begin{note}\label{notecompleteness}
One must be careful about the stipulation that $x \in X$. In particular, suppose our metric space satisfies $X = (a, b)$ with $d$ the Euclidean metric. Then the sequence $(x_n) \subset X$ defined $x_n = a + n^{-1}, \ n \in \mathbb{N}$ is divergent. If we were to let $X = \mathbb{R}$, though, then $x_n \longrightarrow a$ for any $a \in \mathbb{R}$.
\end{note}

Prior to introducing the first lemma regarding convergent sequences, we first define a bounded subset of a metric space:
\begin{defn}[Bounded set, diameter]
Let $(X, d)$ be a metric space and $M \subseteq X$ a (nonempty) subset of $X$. Then $M$ is a \textbf{bounded set} if
\begin{align*}
    \delta(M) = \sup_{x, y \in M}d(x, y) < \infty.
\end{align*}
For a bounded set $M$, $\delta(M)$ is called the \textbf{diameter} of $M$.
\end{defn}

\begin{lm}[Boundedness, uniqueness of convergent sequences]\label{uniquelimit}
Let $X = (X, d)$ be a metric space. Then
\begin{enumerate}
    \item A convergent sequence in $X$ is bounded and its limit is unique.
    \item If $x_n \longrightarrow x$ and $y_n \longrightarrow y$ in $X$, the  $d(x_n, y_n) \longrightarrow 0$.
\end{enumerate}
\begin{proof}
We first prove statement (1). Let $(x_n)$ be a convergent sequence in $X$ with $x_n \longrightarrow x$. Then there exists a $N \in \mathbb{N}$ such that $n > N \implies d(x_n, x) < 1$. Let $k = \max \{ d(x_1, x), \ldots, d(x_N, x), 1\}$. With this $k$, we can say that $d(x_n, x) \leq k$ for every $n \in \mathbb{N}$. Therefore, for every $n, m \in \mathbb{N}$, $d(x_n, x_m) \leq d(x_n, x) + d(x_m, x) \leq 2k$ by the triangle inequality. We conclude $\sup_{n, m \in \mathbb{N}} d(x_n, x_m) \leq 2k$, and so $(x_n)$ is bounded.\newline
For the second part of proof, suppose towards contradiction that we have both $x_n \longrightarrow x$ as well as $x_n \longrightarrow y$ but $x \neq y$. By the triangle inequality we know that
\begin{align*}
    &d(x, y) \leq d(x, x_n) + d(x_n, y), \quad \forall n \in \mathbb{N}\\
    \implies& d(x, y) \leq \lim_{n \to \infty} d(x_n, x) + \lim_{n \to \infty} d(x_n, y) = 0.
\end{align*}
This is a contradiction, since by the axioms of $(X, d)$ a metric space we know $d(x,y) \leq 0 \implies d(x, y) = 0 \implies x = y$.\newline
Next, we prove statement (2), which is a simple consequence of the triangle inequality. In particular,
\begin{align*}
    &d(x_n, y_n) \leq d(x_n, x) + d(x, y) + d(y, y_n)\\
    \implies& d(x_n, y_n) - d(x, y) \leq d(x_n, x) + d(y_n, y).
\end{align*}
Similarly, we have
\begin{align*}
    &d(x, y) \leq d(x, x_n) + d(x_n, y_n) + d(y_n, y)\\
    \implies&d(x, y) - d(x_n, y_n) \leq d(x_n, x) + d(y_n,y).
\end{align*}
And so, together, these two inequalities imply
\begin{align*}
    | d(x_n, y_n) - d(x, y) | \leq d(x_n, x) + d(y_n,y) \longrightarrow 0 \quad \text{as} \quad n \longrightarrow \infty.
\end{align*}
Therefore, we conclude $d(x_n, y_n) \longrightarrow d(x, y)$ (in the Euclidean metric space) as $n \longrightarrow \infty$.
\end{proof}

\end{lm}

Now that we have defined convergent sequences in a metric space $(X, d)$, we expand our consideration to a more general class of sequences.
\begin{defn}[Cauchy criterion, Cauchy sequence]\label{cauchy}
A sequence $(x_n)$ in a metric space $X = (X, d)$ is said to be \textbf{Cauchy} if for every $\varepsilon > 0$, there is an $N_{\varepsilon} \in \mathbb{N}$ such that
\begin{align*}
    d(x_n, x_m) < \varepsilon \qquad \text{for every $m, n > N_{\varepsilon}$}.
\end{align*}
The previous statement is called the \textbf{Cauchy criterion}.
\end{defn}

\begin{defn}[Completeness]
A metric space $X = (X, d)$ is \textbf{complete} if every Cauchy sequence in $X$ converges.
\end{defn}

\begin{examp}\label{realcomplete}
The real line $(\mathbb{R}, d)$ and the complex plane $(\mathbb{C}, d)$ are complete metric spaces.
\end{examp}

\begin{examp}
The metric space $((a, b), d)$, where $d$ is the Euclidean metric, is not complete. To understand why this is the case, see Note \ref{notecompleteness}.
\end{examp}

\begin{examp}
The rational line $(\mathbb{Q}, d)$ ($d$ is the Euclidean metric) is not complete.\footnotemark
\end{examp}\footnotetext{For an interesting example of a Cauchy sequence in $\mathbb{Q}$ that is divergent, see \href{https://www.maa.org/sites/default/files/Moraless200907137.pdf}{Morales, J. V. (2009). Math Bite: $\mathbb{Q}$ Is Not Complete. Mathematics Magazine, 82(4), 293-294.}}

Although, as we have seen, $(x_n) \subset (X, d)$ is Cauchy $\centernot\implies$ $(x_n)$ is convergent, the opposite implication is, in fact, true:
\begin{thm}\label{cauchycomplete}
Every convergent sequence in a metric space $(X, d)$ is a Cauchy sequence.
\end{thm}

\begin{proof}
Let $(x_n) \subset X$ be an arbitrary convergent sequence with $x_n \longrightarrow x$. And take $\varepsilon > 0$ to be arbitrary. Since $(x_n)$ converges, then there exists some $N_{\epsilon}$ such that $n > N_{\epsilon} \implies d(x_n, x) < \varepsilon$. Having established this fact, then for any $n, m > N_{\epsilon}$ it must be true that
\begin{align*}
    d(x_n, x_m) \leq d(x_n, x) + d(x, x_m) < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon.
\end{align*}
Therefore, from the Definition \ref{cauchy}, the sequence $(x_n)$ is Cauchy.
\end{proof}

\begin{thm}\label{closedconvergent}
Let $M$ be a nonempty subset of a metric space $(X,d)$ and $\bar{M}$ its closure. Then:
\begin{enumerate}
    \item $x \in \bar{M}$ is and only if there is a sequence $(x_n) \subset M$ such that $x_n \longrightarrow x$.
    \item $M$ is closed if and only if $(x_n) \subset M$, $x_n \longrightarrow x$ implies that $x \in M$.
\end{enumerate}
\end{thm}
\begin{proof}
We begin with a proof of (1).\newline
$\implies$ Suppose $x \in \bar{M}$. Then there are two cases to consider. First, if $x \in M$, then let $x_n := x$ be the constant sequence. Clearly we have $x_n \longrightarrow x$ since $d(x_n, x) = 0$ for every $n \in \mathbb{N}$. Otherwise, it must be the case that $x$ is a limit point of $M$. Therefore, for every $n \in \mathbb{N}$, there exists an $x_n \in M$ such that $x_n \in B_{n^{-1}}(x) \iff d(x, x_n) < n^{-1}$. This sequence of points $(x_n) \subset M$ satisfies $x_n \longrightarrow x$ since $\lim_{n \to \infty} n^{-1} = 0$. And so we deduce that there is a sequence $(x_n) \subset M$ such that $x_n \longrightarrow x$.\newline
$\impliedby$ Conversely, suppose that there exists a sequence $(x_n) \subset M$ such that $x_n \longrightarrow x$. If $x_n = x$ for any $n \in \mathbb{N}$, then $x \in M \implies x \in \bar{M}$. Otherwise, we have that for every $\varepsilon > 0$, there exists an $N_{\varepsilon} \in \mathbb{N}$ such that $n > N_{\varepsilon} \implies d(x_n, x) <\varepsilon \iff x_n \in B_{\varepsilon}(x)$. But because $x_n \in M$ with $x_n \neq x$ for every $n \in \mathbb{N}$, this implies that $x$ must be a limit point of $M$. By definition of the closure, we deduce $x \in \bar{M}$. \newline
Now we proceed to prove statement (2).\newline
$\implies$ Let us first assume $M$ is closed. We know from (1) that $(x_n) \subset M$ and $x_n \longrightarrow x$ implies $x \in \bar{M}$. But $M$ closed implies $\bar{M} = M$, meaning $(x_n) \subset M$ and $x_n \longrightarrow x$ implies $x \in M$.\newline
$\impliedby$ Conversely, assume $(x_n) \subset M$, $x_n \longrightarrow x$ implies $x \in M$. And let $x$ be an arbitrary point in $\bar{M}$. Then from (1) we have that there exists a sequence $(x_n) \subset M$ with $x_n \longrightarrow x$. By our assumption, this implies $x \in M$. Hence we have shown $\bar{M} = M$, and so $M$ is closed.
\end{proof}

Theorem \ref{closedconvergent} lends itself to the following result, which is important for understanding complete metric spaces:
\begin{thm}\label{closedcomplete}
A subspace $M$ of a complete metric space $X$ is itself complete if and only if the set $M$ is closed in $X$.
\end{thm}
\begin{proof}
$\implies$ Suppose $M \subset X$ is complete. Let us consider an arbitrary $x \in \bar{M}$. Then by Theorem \ref{closedconvergent}, there exists a sequence $(x_n) \subset M$ such that $x_n \longrightarrow x$. By assumption, though, $M$ is complete, and the limit $x$ of the convergent sequence $(x_n)$ is unique (Lemma \ref{uniquelimit}), meaning that it must be true that $x \in M$. Thus $\bar{M} = M$, and so $M$ is closed in $X$.\newline
$\impliedby$ Conversely, suppose that $M$ is closed in $X$. Take $(x_n)$ to be an arbitrary Cauchy sequence in $M$. Since $X$ is a complete metric space, it must be the case that $x_n \longrightarrow x$ for some $x \in X$. By Theorem, \ref{closedconvergent}, this implies that $x \in \bar{M}$. By assumption, though, $\bar{M} = M$, and so we have shown that each Cauchy sequence in $M$ converges to a limit in $M$.
\end{proof}

The final theorem in this section is of great importance when studying continuous maps. Specifically, it relates the concepts of continuity and convergence.
\begin{thm}[Continuous mapping theorem]\label{continuousmapping}
A mapping $T: X \longrightarrow Y$ of a metric space $(X, d)$ into a metric space $(Y, \tilde{d})$ is continuous at a point $x_0$ if and only if
\begin{align*}
    x_n \longrightarrow x_0 \quad \implies \quad Tx_n \longrightarrow Tx_0.
\end{align*}
\end{thm}
\begin{proof}
$\implies$ First suppose that $T$ is continuous at $x_0$. And let $(x_n) \subset X$ be an arbitrary convergent sequence in $X$ that satisfies $x_n \longrightarrow x_0$. By assumption, for each $\varepsilon > 0$, there exists a $\delta > 0$ such that $d(x, x_0) < \delta \implies \tilde{d}(Tx, Tx_0) < \varepsilon$. But since $x_n \longrightarrow x_0$, then there exists a $N_{\delta} \in \mathbb{N}$ such that $n > N_{\delta} \implies d(x_n, x_0) < \delta$. Therefore, it is true that for all $n > N_{\delta}$, $\tilde{d}(Tx_n, Tx_0) < \varepsilon$. We conclude that $Tx_n \longrightarrow Tx_0$.\newline
$\impliedby$ Suppose $x_n \longrightarrow x_0 \implies Tx_n \longrightarrow Tx_0$. And in the direction of contradiction, let us suppose that $T$ is not continuous at $x_0$. This means that there exists some $\varepsilon' > 0$ such that for every $\delta > 0$, there exists a $x_{\delta} \in X$ with $d(x_{\delta}, x_0) < \delta$ but $\tilde{d}(Tx_{\delta},Tx_0) \geq \varepsilon'$. Therefore, we can form the sequence $(x_n) \subset X$ with $d(x_n, x_0) < n^{-1}$ and $d(Tx_n, Tx_0) \geq \varepsilon'$. Patently, since $\lim_{n \to \infty} n^{-1} = 0$, then $x_n \longrightarrow x_0$. However, $d(Tx_n, Tx_0) \geq \varepsilon'$ for every $n \in \mathbb{N}$ implies $\liminf_{n \to \infty} d(Tx_n, Tx_0) \geq \varepsilon' > 0$. This is a contradiction, though, to our assumption that $Tx_n \longrightarrow Tx_0$. We conclude that the map $T: X \longrightarrow Y$ must be continuous at $x_0$.
\end{proof}

\begin{exer}
If a sequence $(x_n)$ in a metric space $X$ is convergent and has limit $x$, show that every subsequence $(x_{n_k})$ of $(x_n)$ is convergent and has the same limit $x$.

\begin{proof}
Suppose $(x_n)$ is an arbitrary convergent sequence in $X$ satisfying $x_n \longrightarrow x$. And suppose that $(x_{n_k})_{k \in \mathbb{N}}$ is an arbitrary subsequence of $(x_n)$.\footnotemark \ Because $(x_n)$ is convergent, then for each $\varepsilon > 0$ there exists an $N_{\varepsilon} \in \mathbb{N}$ such that $n > N_{\varepsilon} \implies d(x_n, x) < \varepsilon$. But this then means that there exists some $K_{\varepsilon} \in \mathbb{N}$ such that $k > K_{\varepsilon} \implies n_k > N_{\varepsilon} \implies d(x_{n_k}, x) < \varepsilon$. Therefore, we conclude that the subsequence $(x_{n_k})$ converges with limit $x$.
\end{proof}
\end{exer}\footnotetext{Notice that we index a subsequence $(x_{n_k})$ of $(x_n)$ by $k$ rather than by $n$. Particularly, $n_1, n_2, \ldots, n_k, \ldots$ defines a sequence of natural numbers such that $n_1 < n_2 < \ldots < n_k < \ldots$.}

\begin{exer}
If $(x_n)$ is Cauchy and has a convergent subsequence, say $x_{n_k} \longrightarrow x$, show that $(x_n)$ is convergent with the limit $x$.
\begin{proof}
Suppose that $(x_n)$ is a Cauchy sequence in metric space $(X, d)$ with convergent subsequence $x_{n_k} \longrightarrow x$. We will show that the sequence $(x_n)$ itself converges to the limit $x$.\newline
To begin, fix $\varepsilon > 0$ to be arbitrary. By our assumption that $(x_n)$ is Cauchy, there exists a $N_{\varepsilon} \in \mathbb{N}$ such that $n, m > N_{\varepsilon} \implies d(x_n, x_m) < \frac{\varepsilon}{2}$. Additionally, since the subsequence $(x_{n_k})$ is convergent, then there exists some $K_{\varepsilon} \in \mathbb{N}$ such that $k > K_{\varepsilon} \implies d(x_{n_k}, x) < \frac{\varepsilon}{2}$.\newline 
Using these two positive integers $N_{\varepsilon}$ and $K_{\varepsilon}$, let us define $M_{\varepsilon} := \max \{ N_{\varepsilon}, n_{K_{\varepsilon}} \}$. Note that because $M_{\varepsilon} \in \mathbb{N}$, then there also exists a $j \in \mathbb{N}$ such that $n_j > M_{\varepsilon}$. Therefore, we have that for all $n > M_{\varepsilon}$,
\begin{align*}
    &d(x_n, x) \leq d(x_n, x_{n_j}) + d(x_{n_j}, x)\\
    &< \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon.
\end{align*}
The inequality on the second line follows from (1) the Cauchy criterion since $n, n_j > M_{\varepsilon} \geq N_{\varepsilon}$ as well as from (2) the convergence of the subsequence $(x_{n_k})$ since $n_j > M_{\varepsilon} \geq n_{K_{\varepsilon}} \implies j > K_{\varepsilon}$.\newline
Because we let $\varepsilon > 0$ be arbitrary, this proves that $x_n \longrightarrow x$.
\end{proof}
\end{exer}

\begin{exer}
Show that every Cauchy sequence is bounded.
\begin{proof}
Let $X = (X, d)$ be a metric space and $(x_n) \subset X$ an arbitrary Cauchy sequence. Then by the Cauchy criterion (taking $\varepsilon = 1$), there exists an $N \in \mathbb{N}$ such that $n, m > N \implies d(x_n, d_m) < 1$. Using this positive integer $N$, let us define $k := \max \{d(x_1, x_{N+1}), \ldots, d(x_N, x_{N+1}), 1\}$. Then for every $n \in \mathbb{N}$, it must be the case that $d(x_n, x_{N+1}) \leq k$. By appealing to the triangle inequality, we deduce that for each $n , m \in \mathbb{N}$, 
\begin{align*}
    d(x_n, x_m) \leq d(x_n, x_{N+1}) + d(x_{N+1}, x_m) \leq 2k.
\end{align*}
We conclude that the Cauchy sequence $(x_n)$ is bounded.
\end{proof}
\end{exer}

\begin{exer}
Is boundedness of a sequence sufficient for the sequence to be Cauchy? How about convergent?\newline
This is not the case. As a counterexample, consider the Euclidean metric space $(\mathbb{R}, d)$ and the sequence $(x_n)$ defined $x_n := (-1)^n$. For each $n, m \in \mathbb{N}$ we have 
\begin{align*}
d(x_n, x_m) = |x_n - x_m| \leq |x_n| + |x_m| = 2,
\end{align*}
and so the sequence is bounded.\newline
We will show that the sequence is not convergent, and thus it is not Cauchy. Consider an arbitrary $x \in \mathbb{R}$ and define $k := \max\{|x - 1|, |x + 1|\}$, which is the maximum of the distances of $x$ to each of $-1$ and $1$. Evidently, it must be the case that $k > 0$. Then for each $\varepsilon < k$ and for every $N_{\varepsilon} \in \mathbb{N}$, there exists a $n > N_{\varepsilon}$ with $d(x_n , x) = k > \varepsilon$. Therefore, it cannot be the case that $x_n \longrightarrow x$. But we chose $x \in \mathbb{R}$ to be arbitrary, and so the sequence $(x_n)$ is divergent.
\end{exer}

\begin{exer}\label{twocauchyconverge}
If $(x_n)$ and $(y_n)$ are Cauchy sequences in a metric space $(X, d)$, show that $(a_n)$, where $a_n = d(x_n, y_n)$, converges.
\begin{proof}
Let $X =(X, d)$ be an arbitrary metric space and $(x_n)$, $(y_n)$ two Cauchy sequences. Take $\varepsilon > 0$ to be arbitrary. Then by the Cauchy criterion, there exists a $N_{\varepsilon}^{(1)} \in \mathbb{N}$ such that $n, m > N_{\varepsilon}^{(1)} \implies d(x_n, x_m) < \frac{\varepsilon}{2}$ as well as a $N_{\varepsilon}^{(2)} \in \mathbb{N}$ such that $n, m > N_{\varepsilon}^{(2)} \implies d(y_n, y_m) < \frac{\varepsilon}{2}$. Using these $N_{\varepsilon}^{(1)}$ and $N_{\varepsilon}^{(e)}$, define $N = \max\{N_{\varepsilon}^{(1)}, N_{\varepsilon}^{(2)}\}$. Then by the triangle inequality inequality, we have that
\begin{align*}
    n, m > N \implies &d(x_n , y_n) \leq d(x_n, x_m) + d(x_m, y_m) + d(y_m, y_n)\\
    \iff& d(x_n, y_n) - d(x_m, y_m) \leq d(x_n, x_m) + d(y_m, y_n)\\
    \iff& d(x_n, y_n) - d(x_m, y_m) < \frac{\varepsilon}{2} +\frac{\varepsilon}{2} = \varepsilon.
\end{align*}
Similarly, 
\begin{align*}
     n, m > N \implies &d(x_m , y_m) \leq d(x_m, x_n) + d(x_n, y_n) + d(y_n, y_m)\\
     \iff&d(x_m , y_m) -  d(x_n, y_n)\leq d(x_m, x_n) + d(y_n, y_m)\\
     \iff& d(x_m , y_m) -  d(x_n, y_n) < \frac{\varepsilon}{2} +\frac{\varepsilon}{2} = \varepsilon.
\end{align*}
Altogether, we have shown
\begin{align*}
    n, m > N \implies |d(x_n, y_n) - d(x_m, y_m)| < \varepsilon.
\end{align*}
Because $\varepsilon > 0$ was taken to be arbitrary, this proves that $(a_n)$ defined $a_n := d(x_n, y_n)$ is a Cauchy sequence in the Euclidean metric space. And since the real line is complete, we conclude that $(a_n)$ converges.
\end{proof}
\end{exer}
    
\begin{exer}
If $d_1$ and $d_2$ are metrics on the same set $X$ and there are positive numbers $a$ and $b$ such that for all $x, y \in X$,
\begin{align*}
    a \cdot d_1(x, y) \leq d_2(x, y) \leq b \cdot d_1(x, y)
\end{align*}
show that the Cauchy sequences in $(X, d_1)$ and $(X, d_2)$ are the same.
\begin{proof}
Start by assuming $(x_n) \subset X$ is a Cauchy sequence in $(X, d_1)$. Then for each $\varepsilon > 0$, there exists an $N_{\varepsilon} \in \mathbb{N}$ such that $n, m > N_{\varepsilon} \implies d_1(x_n, x_m) < \frac{\varepsilon}{b}$. Note that $b, \varepsilon > 0 \implies \frac{\varepsilon}{b} > 0$. But by the properties of $d_1$ and $d_2$, we also know that $n, m > N_{\varepsilon}$ implies
\begin{align*}
    d_2(x_n, x_m) \leq b \cdot d_1(x_n, x_m) < b \cdot \frac{\varepsilon}{b} = \varepsilon.
\end{align*}
Thus, the sequence $(x_n)$ is also a Cauchy sequence in the metric space $(X, d_2)$.\newline
Now, suppose that $(x_n) \subset X$ is a Cauchy sequence in $(X, d_2)$. Accordingly, for each $\varepsilon > 0$ there exists an $N_{\varepsilon} \in \mathbb{N}$ such that $n, m > N_{\varepsilon} \implies d_2(x_n, x_m) < a \cdot \varepsilon$. Just as in the previous argument, $a, \varepsilon > 0 \implies a \cdot \varepsilon$. Therefore, we have that for every $n, m > N_{\varepsilon}$, 
\begin{align*}
    d_1(x_n, x_m) \leq \frac{1}{a} \cdot d_2(x_n, x_m) < \frac{1}{a} \cdot a \cdot \varepsilon = \varepsilon.
\end{align*}
And so we conclude that $(x_n)$ must also be a Cauchy sequence in $(X, d_1)$.\newline
Since we have shown that every Cauchy in $(X, d_1)$ is a Cauchy sequence in $(X, d_2)$ and vice versa, then the Cauchy sequences in $(X, d_1)$ and $(X, d_2)$ are the same. 
\end{proof}
\end{exer}

\subsection{Examples of Complete Metric Spaces}

Prior to diving into examples of complete metric spaces, we present a general overview of how one would prove that an arbitrary metric space $(X, d)$ is complete.
\begin{note}
Suppose we have a metric space $X = (X, d)$ and want to show that $X$ is complete. To do so, we consider an arbitrary Cauchy sequence $(x_n) \subset X$. Using this Cauchy sequence, we 
\begin{enumerate}
    \item Find a candidate $x$ for the limit of $(x_n)$
    \item Prove that this candidate $x$ is in the metric space $X$
    \item Prove $x_n \longrightarrow x$ in the metric space $X$
\end{enumerate}
Since we have shown that an arbitrary Cauchy sequence in $X$ is convergent, then we conclude $X$ is complete.
\end{note}

Let us see this proof technique in action:

\begin{examp}
The Euclidean metric space $(\mathbb{R}^n, d)$ and unitary metric space $(\mathbb{C}^n, d)$ are complete.
\end{examp}

\begin{examp}
The sequence space $\ell^{\infty}$ is complete.
\begin{proof}
Suppose $(x_n)$ is an arbitrary Cauchy sequence in $\ell^{\infty}$. Let $x_n = (\xi_1^{(n)}, \xi_2^{(n)}, \ldots )$ be the $n$th element of our Cauchy sequence. Then by the Cauchy criterion, we know that for each $\varepsilon > 0$, there exists a $N_{\varepsilon} \in \mathbb{N}$ such that $n, m > N_{\varepsilon} \implies d(x_n, x_m) < \varepsilon$. By the definition of metric $d$, we also know that
\begin{align}
    &d(x_n, x_m) < \varepsilon \iff \sup_{i \in \mathbb{N}} | \xi_i^{(n)} - \xi_i^{(m)}| < \varepsilon & (n, m > N_{\varepsilon})\nonumber\\
    &\implies | \xi_i^{(n)} - \xi_i^{(m)}| < \varepsilon \quad \text{$\forall i \in \mathbb{N}$}. & (n, m > N_{\varepsilon})\label{linftycauchy}
\end{align}
That is, $n, m > N_{\varepsilon} \implies | \xi_i^{(n)} - \xi_i^{(m)}| < \varepsilon$ for each $i \in \mathbb{N}$. Therefore, $\left(\xi_i^{(n)} \right)_{n \in \mathbb{N}}$ is a Cauchy sequence in the Euclidean metric space $(\mathbb{R}, d)$ for each $i \in \mathbb{N}$. And since $(\mathbb{R}, d)$ is complete (see Example \ref{realcomplete}), then this means $\xi_i^{(n)} \longrightarrow \xi_i$ for each $i \in \mathbb{N}$, where the limit $\xi_i$ is in $\mathbb{R}$.\newline
From here, we have a candidate for the limit of our Cauchy sequence $(x_n)$, namely $x = (\xi_1, \xi_2, \ldots)$. By the continuous mapping theorem (Theorem \ref{continuousmapping}), we know 
\begin{align*}
    \xi_i^{(m)} \longrightarrow \xi_i \implies  | \xi_i^{(n)} - \xi_i^{(m)}| \longrightarrow | \xi_i^{(n)} - \xi_i|.
\end{align*} 
And from (\ref{linftycauchy}) $n, m > N_{\varepsilon} \implies | \xi_i^{(n)} - \xi_i^{(m)}| < \varepsilon$, which, together with the previous statement, means 
\begin{align}
     n  > N_{\varepsilon} \implies |\xi_i^{(n)} - \xi_i| \leq \varepsilon. \label{linftybound}
\end{align}
Now, by our assumption that $(x_n) \subset \ell^{\infty}$, then for each $n \in \mathbb{N}$ we have $|\xi_i^{(n)}| \leq k_n, \forall i \in \mathbb{N}$ for some $k_n \in \mathbb{R}_+$. That is, the terms in each sequence $x_n$ are bounded by some constant $k_n$. Consequently, by the triangle inequality in the Euclidean metric space,
\begin{align*}
    |\xi_i| \leq |\xi_i - \xi_i^{(n)}| + |\xi_i^{(n)}| \leq \varepsilon + k_n
\end{align*}
for each $n \in \mathbb{N}$ satisfying $n > N_{\varepsilon}$. Because the right-hand side of the inequality does not depend on the particular term $i \in \mathbb{N}$ of the sequence $x$, we have proven 
\begin{align*}
    \sup_{i \in \mathbb{N}} |\xi_i| \leq \varepsilon + k_n \implies x \in \ell^{\infty}.
\end{align*}
Finally, to show that $x_n \longrightarrow x$, recall our statement (\ref{linftybound}) that $n > N_{\varepsilon} \implies |\xi_i^{(n)} - \xi_i| \leq \varepsilon$ for each $i \in \mathbb{N}$. Accordingly, we have $n > N_{\varepsilon} \implies d(x_n, x) = \sup_{i \in \mathbb{N}} |\xi_i^{(n)} - \xi_i| \leq \varepsilon$. Because $\varepsilon > 0$ was chosen arbitrarily, this proves that $x_n \longrightarrow x$.\newline
We conclude that an arbitrary Cauchy sequence $(x_n)$ in $\ell^{\infty}$ converges, and so $\ell^{\infty}$ is complete.
\end{proof}
\end{examp}

\begin{examp}
The space $c$ consists of all convergent sequences $x = (\xi_i)$ of complex numbers, with the metric induced from the space $\ell^{\infty}$. $c$ is complete.
\begin{proof}
$c$ is a closed subset of $\ell^{\infty}$, which implies by Theorem \ref{closedcomplete} that $c$ is complete.
\end{proof}
\end{examp}

\begin{examp}
The sequence space $\ell^p$ is complete for $p$ is fixed and $1 \leq p < \infty$.
\end{examp}

\begin{examp}\label{functioncomplete}
The function space $C[a, b]$ is complete; $[a,b]$ is any given closed interval on $\mathbb{R}$.
\begin{proof}
Let $(x_n)$ be any Cauchy sequence in $C[a, b]$. Then for every $\varepsilon > 0$, there exists a $N_{\varepsilon} \in \mathbb{N}$ such that $n, m > N_{\varepsilon} \implies d(x_n, x_m) = \max_{t \in [a, b]}|x_n(t) - x_m(t)| < \varepsilon$. This implies that for each fixed $t_0 \in [a, b]$, 
\begin{align}\label{uniformconvergence1}
    n, m > N_{\varepsilon} \implies |x_n(t_0) - x_m(t_0)| \leq \max_{t \in [a, b]}|x_n(t) - x_m(t)| < \varepsilon,
\end{align}
and so $(x_n(t_0))$ is a Cauchy sequence in the Euclidean space $(\mathbb{R}, d)$. Because $(\mathbb{R}, d)$ is a complete metric space (Theorem \ref{realcomplete}), then this implies that $x_n(t_0) \longrightarrow x(t_0)$ for each $t_0 \in [a, b]$. Using these individual limits $x(t_0)$, $t_0 \in [a, b]$, we define the function $x(t)$.\newline
And so it remains to show that the candidate limit $x$ is indeed in $C[a, b]$ and $x_n \longrightarrow x$. In order to do so, we use the continuous mapping theorem to say that for each $t_0 \in [a, b]$, 
\begin{align*}
x_m(t_0) \longrightarrow x(t_0) \implies |x_n(t_0) - x_m(t_0)| \longrightarrow |x_n(t_0) - x(t_0)|.
\end{align*}
Therefore, from (\ref{uniformconvergence1}) we get that for each $t_0 \in [a, b]$
\begin{align}\label{uniformconvergence2}
    n > N_{\varepsilon} \implies |x_n(t_0) - x(t_0)| \leq \varepsilon.
\end{align}
Equipped with this result, we claim the proposed limit $x$ is continuous on the interval $[a, b]$. In particular, let us consider an arbitrary $t_0 \in [a, b]$. And take $\tilde{\varepsilon} > 0$ to be arbitrary. Then by (\ref{uniformconvergence2}), there exists a $M_{\tilde{\varepsilon}} \in \mathbb{N}$ such that $n > M_{\tilde{\varepsilon}} \implies |x_n(t) - x(t)| \leq \frac{\tilde{\varepsilon}}{3}$ for all $t \in [a, b]$.
Using this value $M_{\tilde{\varepsilon}}$, choose some $k > M_{\tilde{\varepsilon}}$. Since $x_k$ is continuous, then there exists a $\delta > 0$ such that $|t - t_0| < \delta \implies |x_k(t) - x_k(t_0)| < \frac{\tilde{\varepsilon}}{3}$. Altogether, we have
\begin{align*}
    &|t - t_0| < \delta \implies |x(t) - x(t_0)| \leq |x(t) - x_k(t)| + |x_k(t) - x_k(t_0)| + |x_k(t_0) - x(t_0)|\\
    &\leq \frac{\tilde{\varepsilon}}{3} + \frac{\tilde{\varepsilon}}{3} + \frac{\tilde{\varepsilon}}{3} = \tilde{\varepsilon}.
\end{align*}
This proves that $x$ is continuous on $[a, b]$, meaning $x \in C[a, b]$.\newline
Finally to prove $x_n \longrightarrow x$, we invoke (\ref{uniformconvergence2}) to say 
\begin{align*}
    n > N_{\varepsilon} \implies d(x_n, x) = \max_{t \in [a, b]} |x_n(t) - x(t)| \leq \varepsilon.
\end{align*}
While this may have seemed obvious when we initially stated (\ref{uniformconvergence2}), we could not consider $d(x_n, x)$ since we did not a priori know that $x \in C[a, b]$. Because $\varepsilon > 0$ was chosen to be arbitrary, we conclude that $x_n \longrightarrow x$.\newline
$(x_n)$ was an arbitrary Cauchy sequence, and so we have proven the metric space $C[a,b]$ is complete.
\end{proof}
\end{examp}

Although we assumed our functions $x \in C[a,b]$ are real-valued, this does not need to be the case. That is, we only considered the real $C[a,b]$. The complex $C[a,b]$, consisting of those complex-valued, continuous functions defined on the interval $[a,b] \subset \mathbb{R}$, is also a complete metric space. Note that for the complex $C[a,b]$, the metric $d$ is defined in terms of the complex modulus function rather than the absolute value. The proof is nearly identical to that we presented in Example \ref{functioncomplete}.

\begin{thm}[Uniform convergence]
Convergence $x_n \longrightarrow x$ in the space $C[a,b]$ is \textbf{uniform convergence}. That is, $(x_m)$ converges uniformly on $[a,b]$ to $x$.
\end{thm}
Hence, the metric on $C[a,b]$ describes uniform convergence on $[a, b]$ and, for this reason, is called the \textbf{uniform metric}.

Now that we have given some examples of complete metric spaces, let us turn our attention to examples of incomplete metric spaces:

\begin{examp}
Consider the subset of $C[a, b]$, denoted $\mathcal{P}$, consisting of all polynomials defined on $[a, b]$. Then the metric space $(\mathcal{P}, d)$, where $d$ is the metric induced on $\mathcal{P}$ by $C[a, b]$, is not complete.
\begin{proof}
Let $x$ be an arbitrary continuous function on $[a, b]$ that is not a polynomial (i.e. $x \in C[a, b] \setminus \mathcal{P}$). By the Weierstrass Approximation Theorem (Example \ref{weierstrass}) $\mathcal{P}$ is dense in $C[a, b]$, meaning there exists a sequence $(x_n) \subset P$ such that $x_n \longrightarrow x$. By Theorem \ref{cauchycomplete}, $(x_n)$ a convergent sequence in $C[a, b]$ implies $(x_n)$ is a Cauchy sequence in $C[a, b]$. And because $d$ is the metric induced on $\mathcal{P}$ by $C[a,b]$, then $(x_n)$ is also a Cauchy sequence in $\mathcal{P}$. Therefore, we have given an instance of a Cauchy sequence $(x_n)$ in $(\mathcal{P}, d)$ that is divergent. We conclude that $(\mathcal{P}, d)$ is an incomplete metric space.
\end{proof}
\end{examp}

\begin{examp}
Let $X$ be the set of all continuous, real-valued functions on $[0, 1]$ and let 
\begin{align*}
    d(x, y) := \int_0^1 |x(t) - y(t)| dt.
\end{align*}
The metric space $(X, d)$ is incomplete.
\begin{proof}
Consider the sequence of continuous functions $(x_n) \subset X$ defined as follows
\begin{align*}
    x_n(t) = \begin{cases}
    0 & 0 \leq t \leq \frac{1}{2}\\
    nt - \frac{n}{2} & \frac{1}{2} < t \leq \frac{1}{2} + \frac{1}{n}\\
    1 & \frac{1}{2} + \frac{1}{n} < t \leq 1
    \end{cases}.
\end{align*}
We can compute the distance between $x_n$ and $x_m$, $n > m$ as
\begin{align*}
    d(x_n, x_m) &= \int_0^1 |x_n(t) - x_m(t)|dt = \frac{1}{2}\left(\frac{1}{m} - \frac{1}{n}\right).
\end{align*}
Notice that this integral is simply equal to the area of the triangle with base of length $\frac{1}{m} - \frac{1}{n}$ and height of length $1$.\newline
Now for each $\varepsilon > 0$, there exists an $N_{\varepsilon} \in \mathbb{N}$ such that $\frac{1}{N_{\varepsilon}} < \varepsilon$. And so by our previous computation, for each $n, m > N_{\varepsilon}$, $n > m$ we have
\begin{align*}
  d(x_n, x_m) =  \frac{1}{2}\left(\frac{1}{m} - \frac{1}{n}\right) < \frac{1}{2} \cdot \frac{1}{m}  < \frac{1}{2} \cdot \frac{1}{N_{\varepsilon}} <  \varepsilon.
\end{align*}
We deduce that $(x_n)$ is a Cauchy sequence in the metric space $(X, d)$.\newline
However, we claim that $(x_n)$ is a divergent sequence. Assume towards contradiction that $(x_n)$ converges to a limit $x$ in $X$. Then by splitting the integral over the regions $\left[0, \frac{1}{2} \right]$, $\left(\frac{1}{2},\frac{1}{2} + \frac{1}{n} \right)$, and $\left[\frac{1}{2} + \frac{1}{n}, 1 \right]$, we compute the distance between $x_n$ and $x$ as
\begin{align*}
    d(x_n, x) = \int_0^{\frac{1}{2}}|x(t)| dt + \int_{\frac{1}{2}}^{\frac{1}{2} + \frac{1}{n}}|x(t) - x_n(t)| dt  +  \int_{\frac{1}{2} + \frac{1}{n}}^1|1-x(t)| dt, \quad n \in \mathbb{N}.
\end{align*}
And so by the Dominated Convergence Theorem
\begin{align*}
    d(x_n, x) \longrightarrow 0 \implies \int_0^{\frac{1}{2}}|x(t)| dt + \int_{\frac{1}{2}}^1|1-x(t)| dt = 0.
\end{align*}
However, each of the integrands is strictly nonnegative, which means
\begin{align*}
    \int_0^{\frac{1}{2}}|x(t)| dt = \int_{\frac{1}{2}}^1|1-x(t)| dt = 0.
\end{align*}
And since $x \in X$, then $x$ is continuous on $[0,1]$. This implies
\begin{align*}
    x(t) = \begin{cases}
    0 & 0 \leq t < \frac{1}{2}\\
    1 & \frac{1}{2}< t < 1
    \end{cases}.
\end{align*}

Clearly, though, this violates the continuity of $x$ on $[0, 1]$ as $\lim_{t \to \frac{1}{2}^-} x(t) \neq \lim_{t \to \frac{1}{2}^+} x(t)$. We conclude that the Cauchy sequence $(x_n)$ in the metric space $(X, d)$ does not converge. Thus, $(X, d)$ is incomplete.
\end{proof}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[
  declare function={
    func1(\x)= (\x < 1/2) * (0)   +
              and (\x >= 1/2, \x < 1/2 + 1/5) * (5*\x - 5/2)  +
              (\x >= 1/2 + 1/5) * (1)
   ;
  },
  declare function={
    func2(\x)= (\x < 1/2) * (0)   +
              and (\x >= 1/2, \x < 1/2 + 1/10) * (10*\x - 10/2)  +
              (\x >= 1/2 + 1/10) * (1)
   ;
  }
]
\begin{axis}[xlabel=$t$, xmin=0, xmax=1, domain=0:1, xlabel=$t$, xtick={0, 0.5, 0.6, 0.7, 1}, xticklabels={0,$1/2$,,,1}, ytick={0,1}, samples=1000]
\addplot [semithick] {func1(x)} node [pos=0.6, below right] {$x_m$};
\addplot [semithick, dashed] {func2(x)} node [pos=0.5, above left] {$x_n$};
\end{axis}
\end{tikzpicture} 
\end{center}
\caption*{Two functions $x_n$ and $x_m$ in our Cauchy sequence $(x_n) \subset X$}
\end{figure}
\end{examp}

\begin{exer}\label{subsetlinfty}
Let $M \subset \ell^{\infty}$ be the subspace consisting of all sequences $x = (\xi_n)$ with at most finitely nonzero terms. Find a Cauchy sequence in $M$ which does not converge in $M$, so that $M$ is incomplete.\newline
Consider the sequence $(x_n) \subset M$ defined $x_n := (1, 1/2, \ldots, 1/n, 0, 0, \ldots)$, $n \in \mathbb{N}$. Notice that for $x_n = (\xi_i^{(n)})$, $|\xi_i^{(n)}| \leq 1$ and $\xi_i^{(n)} = 0, \forall i > n$, meaning that $x_n \in M$. This is a Cauchy sequence because for each $\varepsilon > 0$, there exists a $N_{\varepsilon} \in \mathbb{N}$ such that $\frac{1}{N_{\varepsilon}} < \varepsilon$. And so for $n, m > N_{\varepsilon}$, $n \neq m$  we have
\begin{align*}
    d(x_n, x_m) = \sup_{i \in \mathbb{N}}|\xi_i^{(n)} -\xi_i^{(m)}| = \sup_{\substack{i \in \mathbb{N}\\ \min\{n, m\} < i \leq \max\{n, m\}}} i^{-1} = (\min\{n, m\} + 1)^{-1} < \varepsilon.
\end{align*}
Thus, the sequence $(x_n)$ is indeed Cauchy in $M$.\newline However, $(x_n)$ does not converge in $M$. To see why this is the case, we show $x_n \longrightarrow x$ for $x \in \ell^{\infty}\setminus M$. But by the uniqueness of the limit, this would imply that $(x_n)$ cannot converge in $M$. In particular, we claim $x_n \longrightarrow x$ where $x = (1, 1/2, \ldots, 1/n, \ldots)$. Clearly $x=(\xi_i)$ satisfies $|\xi_i| \leq 1$, and so $x \in \ell^{\infty}$; but we also know $|\xi_i| > 0, \forall i \in \mathbb{N}$, so $x \notin M$. For an arbitrary $\varepsilon > 0$, there exists a $N_{\varepsilon} \in \mathbb{N}$ such that $\frac{1}{N_{\varepsilon}} < \varepsilon$. And so for all $n > N_{\varepsilon}$
\begin{align*}
    d(x_n, x) = \sup_{i \in \mathbb{N}}|\xi_i^{(n)} - \xi_i| = \sup_{\substack{i \in \mathbb{N}\\i > n}} i^{-1} = (n+1)^{-1} < \varepsilon.
\end{align*}
This proves that $x_n \longrightarrow x$ in $\ell^{\infty}$. We have given an example of a Cauchy sequence in $M$ that is divergent, and so we conclude that $M$ is incomplete.
\end{exer}

\begin{exer}
Show that $M$ in Exercise \ref{subsetlinfty} is incomplete by using Theorem \ref{closedcomplete}.
\begin{proof}
We wish to show that $M$ is incomplete by showing that the set $M$ is not closed in the metric space $\ell^{\infty}$. To do so, it suffices to find some $x \in \bar{M}$ such that $x \notin M$. An example of such an $x$ is $x = (1, 1/2, \ldots, 1/n, \ldots) \in X \setminus M$ from the previous problem. For every $\varepsilon > 0$, there exists an $N_{\varepsilon} \in \mathbb{N}$ such that $\frac{1}{N_{\varepsilon}} < \varepsilon$. Then for $x_n = (1, 1/2, \ldots, 1/n, 0, \ldots) \in M$ with $n > N_{\varepsilon}$ we have 
\begin{align*}
    d(x, x_n) = (n+1)^{-1} < \varepsilon \iff x_n \in B_{\varepsilon}(x).
\end{align*}
Because $\varepsilon > 0$ was taken to be arbitrary, this proves that $x$ is a limit point of $M$. Thus $\bar{M} \neq M$, which implies by Theorem \ref{closedcomplete} that $M$ is incomplete.
\end{proof}
\end{exer}

\begin{exer}
Show that the subspace $Y \subset C[a,b]$ consisting of all $x \in C[a,b]$ such that $x(a) = x(b)$ is complete.
\begin{proof}
    Let us consider an arbitrary element $y \in \bar{Y}$. By Theorem \ref{closedconvergent}, there exists a sequence $(y_n) \subset Y$ such that $y_n \longrightarrow y$. Suppose towards contradiction that $y(a) \neq y(b)$. Then because $y_n \longrightarrow y$, for each $\varepsilon > 0$ there exists a $N_{\varepsilon} \in \mathbb{N}$ such that
    \begin{align*}
        n > N_{\varepsilon} \implies d(y_n, y) = \sup_{t \in [a,b]} |y_n(t) - y(t)| < \varepsilon.
    \end{align*}
    But this also implies
    \begin{align*}
        &n > N_{\varepsilon} \implies |y_n(a) - y(a)| < \varepsilon, |y_n(b) - y(b)| < \varepsilon.
    \end{align*}
Therefore, the sequence $(y_n(a))$ satisfies $y_n(a) \longrightarrow y(a)$ and $(y_n(b))$ satisfies $y_n(b) \longrightarrow y(b)$ in the Euclidean metric space $(\mathbb{R}, d)$. But because $(y_n) \subset Y$, we know $y_n(a) = y_n(b), \forall n \in \mathbb{N}$. And so we have proven both $y_n(a) \longrightarrow y(a)$ and $y_n(a) \longrightarrow y(b)$ for $y(a) \neq y(b)$. This, though, contradicts the uniqueness of the limit of a convergent sequence (Theorem \ref{uniquelimit}).\newline
Thus $y \in Y$, which implies that $\bar{Y} = Y$ since we chose $y$ to be arbitrary. Finally  by Theorem \ref{closedcomplete}, $Y$ closed in $C[a,b]$ implies $Y$ is a complete metric space.
\end{proof}
\end{exer}

\begin{exer}
Show that a discrete metric space is complete.
\begin{proof}
Let $(X, d)$ be a discrete metric space, and suppose $(x_n) \subset X$ is an arbitrary Cauchy sequence in $X$.\newline
We claim that there exists an $N \in \mathbb{N}$ such that $n, m > N \implies x_n = x_m$. By the Cauchy criterion, there exists an $N_{\varepsilon = 1} \in \mathbb{N}$ such that $n, m > N_{\varepsilon = 1} \implies d(x_n, x_m) < 1$. However, because $X$ is a discrete metric space, $d(x_n, x_m) < 1 \implies d(x_n, x_m) = 0 \implies x_n = x_m$. Thus, $n, m > N = N_{\varepsilon = 1} \implies x_n = x_m$.\newline
Now that we have established this fact, we claim that $x_n \longrightarrow x$ where $x = x_{N + 1} \in X$. To see why this is the case, let $\varepsilon > 0$ be arbitrary. For all $n > N$, the Cauchy sequence $(x_n)$ satisfies
\begin{align*}
    d(x_n, x) = d(x_n, x_{N+1}) = 0 < \varepsilon.
\end{align*}
Because $\varepsilon > 0$ was chosen arbitrarily, we conclude that indeed $x_n \longrightarrow x$. And since $(x_n) \subset X$ was taken to be an arbitrary Cauchy sequence, we have proven that every Cauchy sequence in $X$ is convergent. Thus, $(X,d)$ is a complete metric space.
\end{proof}
\end{exer}

\begin{exer}
Show that in the space $s$ defined in Exercise \ref{sequencespaces} we have $x_n \longrightarrow x$ if and only if $\xi_j^{(n)} \longrightarrow \xi_j$ for all $j \in \mathbb{N}$, where $x_n = (\xi_j^{(n)})$ and $x = (\xi_i)$.
\begin{proof}
$\implies$ Suppose $x_n \longrightarrow x$ and consider an arbitrary $j \in \mathbb{N}$. Then for each $\varepsilon > 0$, there exists a $N_{\varepsilon} \in \mathbb{N}$ such that 
\begin{align*}
   n > N_{\varepsilon} \implies d(x_n, x) = \sum_{i=1}^{\infty} \frac{1}{2^i} \frac{|\xi_i^{(n)} - \xi_i|}{1 + |\xi_i^{(n)} - \xi_i|} < \frac{1}{2^j}\frac{\varepsilon}{1+\varepsilon}.
\end{align*}
But since the individual terms of the infinite series are nonnegative, this also implies 
\begin{align*}
    &\frac{1}{2^j} \frac{|\xi_j^{(n)} - \xi_j|}{1 + |\xi_j^{(n)} - \xi_j|} \leq d(x_n, x) < \frac{1}{2^j}\frac{\varepsilon}{1+\varepsilon}\\
    \iff& \frac{|\xi_j^{(n)} - \xi_j|}{1 + |\xi_j^{(n)} - \xi_j|} < \frac{\varepsilon}{1+\varepsilon}\\
    \iff&  |\xi_j^{(n)} - \xi_j| < \varepsilon.
\end{align*}
The last implication follows from the fact that the function $f(t) = t/(1+t)$ is strictly increasing on the interval $t > 0$. Because $\varepsilon > 0$ was taken to be arbitrary, this proves $\xi_j^{(n)} \longrightarrow \xi_j$ in the Euclidean metric space $(\mathbb{R}, d)$. And $j \in \mathbb{N}$ was also chosen arbitrarily, which implies that this result holds for all $j \in \mathbb{N}$.\newline
$\impliedby$ Conversely, suppose $\xi_j^{(n)} \longrightarrow \xi_j$ for each $j \in \mathbb{N}$. Notice that for all $n \in \mathbb{N}$,
\begin{align*}
    d(x_n, x) = \sum_{i=1}^{\infty} \frac{1}{2^i}\frac{|\xi_i^{(n)} - \xi_i|}{1 + |\xi_i^{(n)} - \xi_i|} < \sum_{i=1}^{\infty}\frac{1}{2^i} = 1.
\end{align*}
Therefore, by the Dominated Convergence Theorem\footnotemark
\begin{align*}
    \lim_{n \to \infty} d(x_n, x) = \lim_{n \to \infty} \sum_{i=1}^{\infty} \frac{1}{2^i}\frac{|\xi_i^{(n)} - \xi_i|}{1 + |\xi_i^{(n)} - \xi_i|} =  \sum_{i=1}^{\infty} \frac{1}{2^i} \left( \lim_{n \to \infty}\frac{|\xi_i^{(n)} - \xi_i|}{1 + |\xi_i^{(n)} - \xi_i|} \right).
\end{align*}
Also, the continuous mapping theorem tells us that for each $j \in \mathbb{N}$,
\begin{align*}
    \xi_j^{(n)} \longrightarrow \xi_j \implies \frac{|\xi_j^{(n)} - \xi_j|}{1 + |\xi_j^{(n)}- \xi_j|} \longrightarrow 0,
\end{align*}
which implies
\begin{align*}
    \lim_{n \to \infty} \frac{|\xi_j^{(n)} - \xi_j|}{1 + |\xi_j^{(n)}- \xi_j|} = 0.
\end{align*}
Finally, we can compute the limit
\begin{align*}
    \lim_{n \to \infty} d(x_n, x) = \sum_{i=1}^{\infty} \frac{1}{2^i} \left( \lim_{n \to \infty}\frac{|\xi_i^{(n)} - \xi_i|}{1 + |\xi_i^{(n)} - \xi_i|} \right) = 0. 
\end{align*}
Hence, we have $x_n \longrightarrow x$.
\end{proof}
\end{exer}\footnotetext{{This is because we can write $\sum_{i=1}^{\infty} \frac{1}{2^i}\frac{|\xi_i^{(n)} - \xi_i|}{1 + |\xi_i^{(n)} - \xi_i|} = \int f_n(i) \ d\mu(i)$}, where $\mu$ is the counting measure on $\mathbb{N}$. And so if we can prove $|f_n(i)| \leq g(i)$ for $g$ satisfying $\int g(i) \ d\mu(i) < \infty$, then we can invoke the Dominated Convergence Theorem to say $\lim_{n \to \infty} \int f_n(i) \ d\mu(i) = \int \lim_{n \to \infty}f_n(i) \  d\mu(i)$.}

\subsection{Completing Metric Spaces}
We already know that the metric space $(\mathbb{Q}, d)$, where $d$ is the Euclidean metric, can be \textbf{completed} by enlarging the rational line to the entire real line $\mathbb{R}$. This begs the question, though, of whether an arbitrary incomplete metric space can be completed in a similar manner.

\begin{defn}[Isometric mapping, isometric spaces]
Let $X = (X, d)$ and $\tilde{X} = (\tilde{X}, \tilde{d})$ be metric spaces. Then
\begin{enumerate}
    \item A mapping $T: X \longrightarrow \tilde{X}$ is said to be \textbf{isometric} or an \textbf{isometry} if $T$ preserves distances, that is, if for all $x, y \in X$
    \begin{align*}
        \tilde{d}(Tx, Ty) = d(x, y).
    \end{align*}
    \item The space $X$ is said to be \textbf{isometric} with the space $\tilde{X}$ if there exists a bijective isometry of $X$ onto $\tilde{X}$. The spaces $X$ and $\tilde{X}$ are then called \textbf{isometric spaces}.
\end{enumerate}
\end{defn}
To parse this definition, isometric spaces may differ at most by the \enquote{nature of their points}, but these points are indistinguishable in terms of the respective metrics. 

\begin{thm}[Completion]
For a metric space $X = (X, d)$ there exists a complete metric space $\hat{X} = (\hat{X}, \hat{d})$ which has a subspace $W$ which isometric with $X$ and is dense in $\hat{X}$. This space $\hat{X}$ is unique except for isometries, that is, if $\tilde{X}$ is any complete metric space having a dense subspace $\tilde{W}$ isometric with $X$, then $\tilde{X}$ and $\hat{X}$ are isometric.
\end{thm}
\begin{proof}
Just as is done by Kreyszig, we break down this proof into a number of smaller steps:
\begin{enumerate}[(a)] 
\item Construct the metric space $\hat{X} = (\hat{X}, \hat{d})$:\newline
Let $(x_n)$ and $(x_n')$ be two Cauchy sequences in the metric space $X$. We define the equivalence relation $\sim$ such that 
\begin{align*}
 (x_n) \sim (x_n') \iff \lim_{n \longrightarrow \infty} d(x_n, x_n') = 0.   
\end{align*}
From Exercise \ref{twocauchyconverge}, we know that the sequence $(a_n)$ defined $a_n := d(x_n, x_n')$ converges, and so limit $\lim_{n \longrightarrow \infty} d(x_n, x_n')$ indeed exists. One can verify that $\sim$ indeed defines an equivalence relation on the set of Cauchy sequences of $X$.\newline
Let $\hat{X}$ be defined to be the set of all equivalence classes under the the equivalence relation $\sim$. Then for any two elements $\hat{x}, \hat{y} \in \hat{X}$, we define the metric $\hat{d}$ on $\hat{X}$ such that
\begin{align*}
    \hat{d}(\hat{x}, \hat{y}) := \lim_{n \to \infty} d(x_n, y_n),
\end{align*}
where $(x_n) \in \hat{x}$ and $(y_n) \in \hat{y}$ are any two representatives of the equivalence classes $\hat{x}$ and $\hat{y}$, respectively. We must show that $\hat{d}$ does not depend on which representatives from the equivalence classes $\hat{x}$ and $\hat{y}$ we select.\newline 
To see why this is indeed the case, suppose $(x_n), (x_n') \in \hat{x}$ and $(y_n), (y_n') \in \hat{y}$. Let $l_1 = \lim_{n \to \infty} d(x_n, y_n)$ and $l_2 = \lim_{n \to \infty} d(x_n', y_n')$. And take $\varepsilon > 0$ to be arbitrary. Then we have
\begin{align*}
    |l_1 - l_2| \leq |d(x_n, y_n) - l_1|  + |d(x_n, y_n)  - d(x_n', y_n')| + |d(x_n', y_n') - l_2|.
\end{align*}
Clearly, we can find an $N \in \mathbb{N}$ sufficiently large such that $n > N$ implies $|d(x_n, y_n) - l_1| < \frac{\varepsilon}{3}$ and $|d(x_n', y_n') - l_2| < \frac{\varepsilon}{3}$. For the middle term, one can invoke the triangle inequality to show
\begin{align*}
    &d(x_n, y_n) \leq d(x_n, x_n') + d(x_n', y_n') + d(y_n', y_n)\\
    \implies&d(x_n, y_n) - d(x_n', y_n') \leq d(x_n, x_n') + d(y_n', y_n)
\end{align*}
and similarly $d(x_n', y_n') - d(x_n, y_n) \leq d(x_n, x_n') + d(y_n', y_n)$. Together, these statements imply \begin{align*}
    |d(x_n', y_n') - d(x_n, y_n)| \leq d(x_n, x_n') + d(y_n', y_n).
\end{align*}
But because $(x_n) \sim (x_n')$ and $(y_n) \sim (y_n')$ we know it must the case that $\lim_{n \to \infty} d(x_n, x_n') = \lim_{n \to \infty} d(y_n, y_n') = 0$. Therefore, we can also find an $M \in \mathbb{N}$ sufficiently large so that $n > M$ implies
\begin{align*}
    |d(x_n', y_n') - d(x_n, y_n)| \leq d(x_n, x_n') + d(y_n', y_n) < \frac{\varepsilon}{3}.
\end{align*}
Altogether, we have proven that for $n > \max\{N, M\}$, then
\begin{align*}
    |l_1 - l_2| < \varepsilon.
\end{align*}
Since $\varepsilon > 0$ was chosen to be arbitrary, we deduce
\begin{align*}
    |l_1 - l_2| \leq 0 \implies |l_1 - l_2| = 0 \iff l_1 = l_2.
\end{align*}\newline
Now that we have shown that $\hat{d}$ is well-defined, it remains to prove that $\hat{d}$ is indeed a metric. From our previous note, we know that $\hat{d}(\hat{x}, \hat{y}) = \lim_{n \to \infty} d(x_n, y_n)$ exists as well as $d(x_n, y_n) \geq 0, \forall n \in \mathbb{N} \implies \hat{d}(\hat{x}, \hat{y}) = \lim_{n \to \infty} d(x_n, y_n) \geq 0$. For the second axiom, let us first suppose that $\hat{d}(\hat{x}, \hat{y}) = \lim_{n \to \infty} d(x_n, y_n) = 0$. By definition of our equivalence relation $\sim$, this means $(x_n) \sim (y_n) \implies \hat{x} = \hat{y}$. Conversely, $\tilde{d}(\hat{x}, \hat{x}) = \lim_{n\to \infty} d(x_n, x_n) = 0$. For the third axiom, we have $\tilde{d}(\hat{x}, \hat{y}) = \lim_{n \to \infty} d(x_n, y_n) = \lim_{n \to \infty} d(y_n, x_n) = \tilde{d}(\hat{y}, \hat{x})$. Finally, for the triangle inequality, we know that since $(X, d)$ is a metric space, then it holds that for every $n \in \mathbb{N}$,
\begin{align*}
    d(x_n, z_n) \leq d(x_n, y_n) + d(y_n, z_n),
\end{align*}
where $(x_n) \in \hat{x}$, $(y_n) \in \hat{y}$, and $(z_n) \in \hat{z}$. Taking the limit on both sides of the inequality, 
\begin{align*}
    \hat{d}(\hat{x}, \hat{z}) = \lim_{n \to \infty} d(x_n, z_n) \leq \lim_{n \to \infty} d(x_n, y_n) + \lim_{n \to \infty} d(y_n, z_n) = \hat{d}(\hat{x}, \hat{y}) + \hat{d}(\hat{y}, \hat{z}).
\end{align*}\newline
And so we have proven $(\hat{X}, \hat{d})$ is indeed a metric space.

\item There exists an isometry $T: X \longrightarrow W \subset \hat{X}$, where $\bar{W} = \hat{X}$:\newline
Define $T: X \longrightarrow \hat{X}$ such that $Tx = \hat{x}$, where $\hat{x}$ is equivalence class which contains the constant sequence $(x, x, \ldots)$. Clearly $(x_n) = (x, x, \ldots) \in \hat{X}$ because $d(x_n, x_m) = d(x, x) = 0$ for every $n, m \in \mathbb{N}$. For our set $W$, we let $W = T(X)$, where $T(X)$ denotes the image of $X$ under the map $T$.\newline
Now we show that $X$ is isometric with $W$ by proving that $T: X \longrightarrow W$ is a bijective isometry. We see that $T$ is an isometry because for $Tx = (x_n), Ty = (y_n)$, we have $\hat{d}(Tx, Ty) = \lim_{n \to \infty}d(x_n, y_n) = \lim_{n \to \infty}d(x, y) = d(x, y)$. $T$ is injective because $Tx = Ty \iff \hat{d}(Tx, Ty) = 0 \iff d(x, y) = 0 \iff x=y$. And $T: X \longrightarrow W$ is surjective since we have defined $W = T(X)$.\newline
Finally, we must prove that $\bar{W} = \hat{X}$, that is, $W$ is a dense subset of the metric space $\hat{X}$. To prove that this, we show that every point in $\hat{X}$ is a limit point of $W$. In particular, consider an arbitrary $\hat{x} \in \hat{X}$. And let $(x_n)$ be an arbitrary element of the equivalence class $\hat{x}$. Take $\varepsilon > 0$ to be arbitrary. Since $(x_n)$ is a Cauchy sequence in $X$, there exists an $N \in \mathbb{N}$ such that
\begin{align*}
    n > N \implies d(x_n, x_N) < \frac{\varepsilon}{2}.
\end{align*}
Clearly, it is also the case that the constant sequence $(x_N, x_N, \ldots) \in W$. Let $\hat{x}_N$ denote the equivalence class containing this sequence. Then we have
\begin{align*}
    \hat{d}(\hat{x}, \hat{x}_N) = \lim_{n \to \infty}d(x_n, x_N) \leq \frac{\varepsilon}{2} < \varepsilon.
\end{align*}
Because $\varepsilon > 0$ was chosen arbitrarily, this proves that $\hat{x}$ is a limit point of $X$. And because $\hat{x}$ was taken to be arbitrary as well, we conclude $\bar{W} = \hat{X}$.

\item $\hat{X}$ is complete:\newline
Let $(\hat{x}_n)$ be an arbitrary Cauchy sequence in $\hat{X}$. Because $W$ is dense in $\hat{X}$, then for every $n \in \mathbb{N}$, we can find a $\hat{z}_n \in W$ which satisfies
\begin{align*}
    \hat{d}(\hat{x}_n, \hat{z}_n) < \frac{1}{n}.
\end{align*}
From here, we claim that the sequence $(\hat{z}_n)$ is Cauchy in $\hat{X}$. Take $\varepsilon > 0$ to be arbitrary. Then we can choose $N \in \mathbb{N}$ such that $\frac{1}{N} < \frac{\varepsilon}{3}$ and $n, m > N \implies \hat{d}(\hat{x}_n, \hat{x}_m) < \frac{\varepsilon}{3}$ (because $(\hat{x}_n)$ is a Cauchy sequence). By the triangle inequality, this gives us 
\begin{align*}
    n, m > N \implies \hat{d}(\hat{z}_n, \hat{z}_m) \leq \hat{d}(\hat{z}_n, \hat{x}_n) + \hat{d}(\hat{x}_n, \hat{x}_m) + \hat{d}(\hat{x}_m, \hat{z}_m) < \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3} = \varepsilon.
\end{align*}
And so because $\varepsilon > 0$ was chosen to be arbitrary, we deduce that $(\hat{z}_n)$ is indeed a Cauchy sequence.\newline Now, since $(\hat{z}_n) \subset W$ and $T: X \longrightarrow W$ is injective, then we can define the sequence $(z_n) \subset X$, where $z_n := T^{-1}\hat{z}_n$. But by our previous result that $(\hat{z}_n)$ is Cauchy and since $T$ is an isometry, then the sequence $(z_n)$ is Cauchy in the metric space $X$. That is, we have that $(z_n)$ belongs to some equivalence class $\hat{x} \in \hat{X}$. This $\hat{x}$ is our candidate for the limit of $(\hat{x}_n)$.\newline
It remains only to show that $\hat{x}_n \longrightarrow \hat{x}$ in the metric space $(\hat{X}, \hat{d})$. To start, let us take $\varepsilon > 0$ be arbitrary. And choose $N \in \mathbb{N}$ such that $\frac{1}{N} < \varepsilon$ as well as $n, m > N \implies d(z_n, z_m) < \frac{\varepsilon}{2}$, which we can do since $(z_n)$ is a Cauchy sequence. Then we get that for $n > N$
\begin{align*}
    \hat{d}(\hat{x}_n, \hat{x}) &\leq \hat{d}(\hat{x}_n, \hat{z}_n) + \hat{d}(\hat{z}_n, \hat{x})\\
    &< \frac{1}{n} + \hat{d}(\hat{z}_n, \hat{x})\\
    &< \frac{\varepsilon}{2} + \hat{d}(\hat{z}_n, \hat{x}).
\end{align*}
And since $\hat{z}_n \in W$ then we have $(z_n, z_n, \ldots) \in \hat{z}_n$, where $z_n = T^{-1}\hat{z}_n$ as we previously defined. By the definitions of $\hat{d}$ and $\hat{x}$, this implies
\begin{align*}
    \hat{d}(\hat{x}_n, \hat{x}) &< \frac{\varepsilon}{2} + \lim_{m \to \infty}d(z_n, z_m)\\
    &= \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon.
\end{align*}
Because $\varepsilon > 0$ was taken to be arbitrary, we have proven that $\hat{x}_n \longrightarrow \hat{x}$. And as $(\hat{x}_n)$ was chosen to be an arbitrary Cauchy sequence in $\hat{X}$, then we conclude that the metric space $(\hat{X}, \hat{d})$ is complete.

\item $\hat{X}$ is unique except for isometries:\newline
Finally, we must prove that if $(\tilde{X}, \tilde{d})$ is another complete metric space with a dense subset $\tilde{W}$ that is isometric with $X$, then $\tilde{X}$ and $\hat{X}$ are isometric. Let $\tilde{x}$ and $\tilde{y}$ be arbitrary elements of $\tilde{X}$. By Theorem \ref{closedconvergent}, there exists sequences $(\tilde{x}_n)$ and $(\tilde{y}_n)$ in $\tilde{W}$ such that $\tilde{x}_n \longrightarrow \tilde{x}$ and $\tilde{y}_n \longrightarrow \tilde{y}$. By the triangle inequality, we have that 
\begin{align*}
    |\tilde{d}(\tilde{x}, \tilde{y}) - \tilde{d}(\tilde{x}_n, \tilde{y}_n)| \leq \tilde{d}(\tilde{x}_n, \tilde{x}) + \tilde{d}(\tilde{y}_n, \tilde{y}) \longrightarrow 0,
\end{align*}
which proves that
\begin{align*}
    \tilde{d}(\tilde{x}, \tilde{y}) = \lim_{n \longrightarrow \infty} \tilde{d}(\tilde{x}_n, \tilde{y}_n).
\end{align*}
Since $\tilde{W}$ is isometric with $X$ and $X$ is isometric with $\hat{W}$, then $\tilde{W}$ is isometric with $\hat{W}$. And because $\bar{W} = \hat{X}$, then the distances on $\hat{X}$ and $\tilde{X}$ must be identical. Thus, $\hat{X}$ and $\tilde{X}$ must be isometric spaces.
\end{enumerate}
\end{proof}

\end{document}